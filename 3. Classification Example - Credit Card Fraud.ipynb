{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类实例-信用卡欺诈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础知识：逻辑回归与交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 线性回归预测的是一个连续值\n",
    "* 逻辑回归给出的`是`或`否`的回答\n",
    "\n",
    "**Sigmoid函数：** \n",
    "* y = 1 / [1 + e ^ (-x)] （将 (-∞, +∞) 映射到 (0, 1) ）\n",
    "* Sigmoid函数是一个概率分布函数，给定某个输入，将输出为一个概率值\n",
    "\n",
    "**逻辑回归损失函数**\n",
    "* `平方差`所惩罚的是与损失为同一数量级的情形\n",
    "* 对于分类问题，最好使用`交叉熵`损失函数，交叉熵会输出一个更大的“损失”  \n",
    "\n",
    "    交叉熵刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。  \n",
    "    假设`概率分布p`为期望输出，`概率分布q`为实际输出，`H(p,q)`为交叉熵，则：`H(p,q) = -Σ[p(x)logq(x)]`  \n",
    "\n",
    "    在Pytorch中，使用`nn.BCELoss()`来计算二元交叉熵损失（在线性回归中，使用`nn.MSELoss()`计算均方差损失）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./dataset/Credit.csv', header=None)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into X and y\n",
    "X = data.iloc[:, :-1] # 所有的行，前15列为特征\n",
    "print(X.head())\n",
    "print(X.info())\n",
    "Y = data.iloc[:, -1]  # 所有的行，最后一列为标签\n",
    "print(Y.unique())\n",
    "Y = data.iloc[:, -1].replace(-1, 0)  # 将-1转换为0，则标签全部转换为0和1\n",
    "# 或者使用下面的方法建立映射\n",
    "# Y = data.iloc[:, -1].map({-1: 0, 1: 1})\n",
    "print(Y.unique())\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X = torch.from_numpy(X.values).float()  # 或者使用.type(torch.FloatTensor)或.type(torch.float32)\n",
    "Y = torch.from_numpy(Y.values.reshape(-1, 1)).float()\n",
    "print(X.dtype, Y.dtype)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = nn.Sequential(  # 定义一个序列模型，Sequential可以将多个层顺序地链接在一起\n",
    "                    nn.Linear(15, 1),   # 第一层为线性层，输入特征为15，输出特征为1\n",
    "                    nn.Sigmoid()        # 第二层为Sigmoid激活函数\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = nn.BCELoss() # 二分类交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0001)  # Adam优化器\n",
    "# Adam优化器:自适应矩估计，可以考虑前几次的梯度，不仅考虑当前的梯度，还考虑之前的梯度\n",
    "# SGD优化器:随机梯度下降，不会考虑前几次的梯度，只考虑当前的梯度\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小批量训练\n",
    "# Define the batch size\n",
    "batches = 16\n",
    "num_of_batches = 653//16 # //表示整除\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in range(num_of_batches):     # 按照批次进行训练\n",
    "        start = batch*batches               # 每个批次的起始索引\n",
    "        end = start + batches               # 每个批次的结束索引\n",
    "        x = X[start: end]\n",
    "        y = Y[start: end]\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute loss: BCELoss expects the target to be between 0 and 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Gradient reset\n",
    "        opt.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型的参数\n",
    "model.state_dict() # sigmoid(w1*x1 + w2*x2 + ... + w15*x15 + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model: 计算准确率\n",
    "accuracy = ((model(X).data.numpy() > 0.5).astype(int) == Y.numpy()).mean()\n",
    "# model(X).data.numpy() > 0.5: 将模型的输出值大于0.5的转换为1，小于0.5的转换为0\n",
    "# .astype(int): 将布尔值转换为整数\n",
    "# == Y.numpy(): 将转换后的值与真实值进行比较\n",
    "# .mean(): 计算均值，即准确率\n",
    "print(str(accuracy * 100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
