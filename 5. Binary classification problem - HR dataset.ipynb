{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二分类问题-HR数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>part</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years   part  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./dataset/HR.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   part                   14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# 数据基本信息\n",
    "print(data.info())\n",
    "print(data.part.unique())\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>...</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  IT  RandD  \\\n",
       "0                   3              0     1                      0   0      0   \n",
       "1                   6              0     1                      0   0      0   \n",
       "2                   4              0     1                      0   0      0   \n",
       "3                   5              0     1                      0   0      0   \n",
       "4                   3              0     1                      0   0      0   \n",
       "\n",
       "   ...  hr  management  marketing  product_mng  sales  support  technical  \\\n",
       "0  ...   0           0          0            0      1        0          0   \n",
       "1  ...   0           0          0            0      1        0          0   \n",
       "2  ...   0           0          0            0      1        0          0   \n",
       "3  ...   0           0          0            0      1        0          0   \n",
       "4  ...   0           0          0            0      1        0          0   \n",
       "\n",
       "   high  low  medium  \n",
       "0     0    1       0  \n",
       "1     0    0       1  \n",
       "2     0    0       1  \n",
       "3     0    1       0  \n",
       "4     0    1       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 简单的数据分析\n",
    "# data.groupby(['salary', 'part']).size()\n",
    "# .get_dummies()方法可以将分类数据转换为one-hot编码\n",
    "data = data.join(pd.get_dummies(data.part).astype(int)).join(pd.get_dummies(data.salary).astype(int))\n",
    "# 删除原来的分类数据\n",
    "data.drop(columns=['part', 'salary'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   IT                     14999 non-null  int32  \n",
      " 9   RandD                  14999 non-null  int32  \n",
      " 10  accounting             14999 non-null  int32  \n",
      " 11  hr                     14999 non-null  int32  \n",
      " 12  management             14999 non-null  int32  \n",
      " 13  marketing              14999 non-null  int32  \n",
      " 14  product_mng            14999 non-null  int32  \n",
      " 15  sales                  14999 non-null  int32  \n",
      " 16  support                14999 non-null  int32  \n",
      " 17  technical              14999 non-null  int32  \n",
      " 18  high                   14999 non-null  int32  \n",
      " 19  low                    14999 non-null  int32  \n",
      " 20  medium                 14999 non-null  int32  \n",
      "dtypes: float64(2), int32(13), int64(6)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "0    11428\n",
      "1     3571\n",
      "Name: count, dtype: int64\n",
      "0.7619174611640777\n"
     ]
    }
   ],
   "source": [
    "# 查看离职率\n",
    "print(data.left.value_counts())\n",
    "# 全部预测为不离职\n",
    "print(data.left.value_counts()[0] / data.left.value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 1])\n"
     ]
    }
   ],
   "source": [
    "# 处理结果数据\n",
    "Y_data = data.left.values.reshape(-1, 1)\n",
    "Y = torch.from_numpy(Y_data).type(torch.FloatTensor)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 20])\n"
     ]
    }
   ],
   "source": [
    "# 处理特征数据\n",
    "# 使用列表推导式，获取除了'left'列之外的所有列\n",
    "# [c for c in data.columns if c != 'left']\n",
    "# 使用.values方法，将DataFrame转换为numpy数组\n",
    "X_data = data[[c for c in data.columns if c != 'left']].values\n",
    "X = torch.from_numpy(X_data).type(torch.FloatTensor)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# # 自定义模型：逻辑回归模型\n",
    "# class Logistic(nn.Module):  # 继承nn.Module\n",
    "#     def __init__(self):     # 初始化所有的层\n",
    "#         super().__init__()  # 继承父类中所有的属性和方法\n",
    "#         self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "#         self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "#         self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "#         self.activate = nn.ReLU()       # 定义ReLU激活函数\n",
    "#         self.sigmoid = nn.Sigmoid()     # 定义Sigmoid激活函数\n",
    "#     def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "#         x = self.lin_1(input)   # 将输入数据传入第一层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_2(x)       # 将激活后的数据传入第二层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_3(x)       # 将激活后的数据传入第三层线性层\n",
    "#         x = self.sigmoid(x)     # Sigmoid激活函数\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型改写\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # 函数式API，调用方便使代码更简洁\n",
    "class Logistic(nn.Module):  # 继承nn.Module\n",
    "    def __init__(self):     # 初始化所有的层\n",
    "        super().__init__()  # 继承父类中所有的属性和方法\n",
    "        self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "        self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "        self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "    def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "        x = F.relu(self.lin_1(input))   # 将输入数据传入第一层线性层，并使用ReLU激活函数\n",
    "        x = F.relu(self.lin_2(x))       # 将激活后的数据传入第二层线性层，并使用ReLU激活函数\n",
    "        x = F.sigmoid(self.lin_3(x))     # 将激活后的数据传入第三层线性层，并使用Sigmoid激活函数\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Logistic(\n",
      "  (lin_1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (lin_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_3): Linear(in_features=64, out_features=1, bias=True)\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# 封装模型和优化器的创建，提高代码复用性\n",
    "lr = 0.0001\n",
    "def get_model():\n",
    "    model = Logistic()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, opt\n",
    "print(get_model())\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集，分批次进行训练\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 手动分批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.718896746635437\n",
      "epoch: 1     loss: 0.7514970898628235\n",
      "epoch: 2     loss: 0.7525617480278015\n",
      "epoch: 3     loss: 0.7574713826179504\n",
      "epoch: 4     loss: 0.7550879716873169\n",
      "epoch: 5     loss: 0.7513564229011536\n",
      "epoch: 6     loss: 0.7469230890274048\n",
      "epoch: 7     loss: 0.7417653799057007\n",
      "epoch: 8     loss: 0.732473611831665\n",
      "epoch: 9     loss: 0.7307769060134888\n",
      "epoch: 10     loss: 0.7196688055992126\n",
      "epoch: 11     loss: 0.7119090557098389\n",
      "epoch: 12     loss: 0.7045210599899292\n",
      "epoch: 13     loss: 0.6966570019721985\n",
      "epoch: 14     loss: 0.6871347427368164\n",
      "epoch: 15     loss: 0.6803553700447083\n",
      "epoch: 16     loss: 0.6704649925231934\n",
      "epoch: 17     loss: 0.6613579392433167\n",
      "epoch: 18     loss: 0.6529034376144409\n",
      "epoch: 19     loss: 0.6434773802757263\n",
      "epoch: 20     loss: 0.6502538919448853\n",
      "epoch: 21     loss: 0.6370353698730469\n",
      "epoch: 22     loss: 0.6267071962356567\n",
      "epoch: 23     loss: 0.6172723174095154\n",
      "epoch: 24     loss: 0.607367753982544\n",
      "epoch: 25     loss: 0.5991159677505493\n",
      "epoch: 26     loss: 0.5919222235679626\n",
      "epoch: 27     loss: 0.5809769630432129\n",
      "epoch: 28     loss: 0.5806130170822144\n",
      "epoch: 29     loss: 0.5680078268051147\n",
      "epoch: 30     loss: 0.566339373588562\n",
      "epoch: 31     loss: 0.5611129403114319\n",
      "epoch: 32     loss: 0.5584841370582581\n",
      "epoch: 33     loss: 0.5590471029281616\n",
      "epoch: 34     loss: 0.5593146085739136\n",
      "epoch: 35     loss: 0.5556732416152954\n",
      "epoch: 36     loss: 0.5634657144546509\n",
      "epoch: 37     loss: 0.5572994947433472\n",
      "epoch: 38     loss: 0.5545600652694702\n",
      "epoch: 39     loss: 0.5538899898529053\n",
      "epoch: 40     loss: 0.552009642124176\n",
      "epoch: 41     loss: 0.5532849431037903\n",
      "epoch: 42     loss: 0.5518495440483093\n",
      "epoch: 43     loss: 0.5506044030189514\n",
      "epoch: 44     loss: 0.5498287081718445\n",
      "epoch: 45     loss: 0.5479270815849304\n",
      "epoch: 46     loss: 0.5491042733192444\n",
      "epoch: 47     loss: 0.5478233098983765\n",
      "epoch: 48     loss: 0.5473445057868958\n",
      "epoch: 49     loss: 0.547042727470398\n",
      "epoch: 50     loss: 0.5466651320457458\n",
      "epoch: 51     loss: 0.5459827780723572\n",
      "epoch: 52     loss: 0.5463705658912659\n",
      "epoch: 53     loss: 0.5461278557777405\n",
      "epoch: 54     loss: 0.5454713106155396\n",
      "epoch: 55     loss: 0.5463765263557434\n",
      "epoch: 56     loss: 0.5461418628692627\n",
      "epoch: 57     loss: 0.546027421951294\n",
      "epoch: 58     loss: 0.5454822778701782\n",
      "epoch: 59     loss: 0.5429791212081909\n",
      "epoch: 60     loss: 0.5455270409584045\n",
      "epoch: 61     loss: 0.5453419089317322\n",
      "epoch: 62     loss: 0.5424947738647461\n",
      "epoch: 63     loss: 0.5439351797103882\n",
      "epoch: 64     loss: 0.5449492931365967\n",
      "epoch: 65     loss: 0.5439755916595459\n",
      "epoch: 66     loss: 0.540756344795227\n",
      "epoch: 67     loss: 0.542715311050415\n",
      "epoch: 68     loss: 0.5436948537826538\n",
      "epoch: 69     loss: 0.5420103073120117\n",
      "epoch: 70     loss: 0.541449785232544\n",
      "epoch: 71     loss: 0.5414117574691772\n",
      "epoch: 72     loss: 0.5374898314476013\n",
      "epoch: 73     loss: 0.5427795648574829\n",
      "epoch: 74     loss: 0.5357452034950256\n",
      "epoch: 75     loss: 0.5376170873641968\n",
      "epoch: 76     loss: 0.534697413444519\n",
      "epoch: 77     loss: 0.5355953574180603\n",
      "epoch: 78     loss: 0.5340298414230347\n",
      "epoch: 79     loss: 0.5334330201148987\n",
      "epoch: 80     loss: 0.5317654609680176\n",
      "epoch: 81     loss: 0.5303220152854919\n",
      "epoch: 82     loss: 0.5312502384185791\n",
      "epoch: 83     loss: 0.5283936858177185\n",
      "epoch: 84     loss: 0.5287706851959229\n",
      "epoch: 85     loss: 0.5269991159439087\n",
      "epoch: 86     loss: 0.5267230868339539\n",
      "epoch: 87     loss: 0.5258618593215942\n",
      "epoch: 88     loss: 0.5255265831947327\n",
      "epoch: 89     loss: 0.5223339200019836\n",
      "epoch: 90     loss: 0.5230305194854736\n",
      "epoch: 91     loss: 0.5226286053657532\n",
      "epoch: 92     loss: 0.5213724970817566\n",
      "epoch: 93     loss: 0.5202765464782715\n",
      "epoch: 94     loss: 0.5201568603515625\n",
      "epoch: 95     loss: 0.519136905670166\n",
      "epoch: 96     loss: 0.5286397933959961\n",
      "epoch: 97     loss: 0.5197793245315552\n",
      "epoch: 98     loss: 0.5185388326644897\n",
      "epoch: 99     loss: 0.517177402973175\n"
     ]
    }
   ],
   "source": [
    "# 分批次循环训练\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):     # 按照批次进行训练\n",
    "        start = i * batch              # 每个批次的起始索引\n",
    "        end = start + batch                # 每个批次的结束索引\n",
    "        x = X[start: end]\n",
    "        y = Y[start: end]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute loss: BCELoss expects the target to be between 0 and 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Gradient reset\n",
    "        opt.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用dataset重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch有一个抽象的Dataset类。Dataset可以是任何具有__len__函数和__getitem__作为对其进行索引的方法的函数。PyTorch的TensorDataset是一个包装张量的Dataset。通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。这将使我们在训练的同一行中更容易访问自变量和因变量。下面将自定义HRDataset类创建为的Dataset的子类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "# print(HRdataset[2: 5])\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6838170289993286\n",
      "epoch: 1     loss: 0.6829855442047119\n",
      "epoch: 2     loss: 0.681487500667572\n",
      "epoch: 3     loss: 0.6629778146743774\n",
      "epoch: 4     loss: 0.6565048098564148\n",
      "epoch: 5     loss: 0.6490548849105835\n",
      "epoch: 6     loss: 0.6417019367218018\n",
      "epoch: 7     loss: 0.6344111561775208\n",
      "epoch: 8     loss: 0.6272223591804504\n",
      "epoch: 9     loss: 0.6201733946800232\n",
      "epoch: 10     loss: 0.6132480502128601\n",
      "epoch: 11     loss: 0.6087155342102051\n",
      "epoch: 12     loss: 0.6328052282333374\n",
      "epoch: 13     loss: 0.6207132339477539\n",
      "epoch: 14     loss: 0.6116995811462402\n",
      "epoch: 15     loss: 0.6011788845062256\n",
      "epoch: 16     loss: 0.5948858857154846\n",
      "epoch: 17     loss: 0.5890817642211914\n",
      "epoch: 18     loss: 0.5841034650802612\n",
      "epoch: 19     loss: 0.5794222354888916\n",
      "epoch: 20     loss: 0.5751633644104004\n",
      "epoch: 21     loss: 0.571726381778717\n",
      "epoch: 22     loss: 0.5682868957519531\n",
      "epoch: 23     loss: 0.5656812191009521\n",
      "epoch: 24     loss: 0.5632073283195496\n",
      "epoch: 25     loss: 0.561542272567749\n",
      "epoch: 26     loss: 0.559765100479126\n",
      "epoch: 27     loss: 0.5586661696434021\n",
      "epoch: 28     loss: 0.5576469302177429\n",
      "epoch: 29     loss: 0.556988537311554\n",
      "epoch: 30     loss: 0.5566020011901855\n",
      "epoch: 31     loss: 0.5558080673217773\n",
      "epoch: 32     loss: 0.5562061667442322\n",
      "epoch: 33     loss: 0.5562853813171387\n",
      "epoch: 34     loss: 0.5566408038139343\n",
      "epoch: 35     loss: 0.5560081005096436\n",
      "epoch: 36     loss: 0.5563488602638245\n",
      "epoch: 37     loss: 0.5565265417098999\n",
      "epoch: 38     loss: 0.5566640496253967\n",
      "epoch: 39     loss: 0.5564092993736267\n",
      "epoch: 40     loss: 0.553992748260498\n",
      "epoch: 41     loss: 0.5562736988067627\n",
      "epoch: 42     loss: 0.559081494808197\n",
      "epoch: 43     loss: 0.5553269386291504\n",
      "epoch: 44     loss: 0.5560132265090942\n",
      "epoch: 45     loss: 0.5557132363319397\n",
      "epoch: 46     loss: 0.5555237531661987\n",
      "epoch: 47     loss: 0.5557482242584229\n",
      "epoch: 48     loss: 0.555440366268158\n",
      "epoch: 49     loss: 0.5588878393173218\n",
      "epoch: 50     loss: 0.5535663962364197\n",
      "epoch: 51     loss: 0.5515199899673462\n",
      "epoch: 52     loss: 0.5558112859725952\n",
      "epoch: 53     loss: 0.5555520057678223\n",
      "epoch: 54     loss: 0.5577684640884399\n",
      "epoch: 55     loss: 0.5549432039260864\n",
      "epoch: 56     loss: 0.5549651980400085\n",
      "epoch: 57     loss: 0.5544722080230713\n",
      "epoch: 58     loss: 0.5544561147689819\n",
      "epoch: 59     loss: 0.5539712309837341\n",
      "epoch: 60     loss: 0.5530417561531067\n",
      "epoch: 61     loss: 0.5498523116111755\n",
      "epoch: 62     loss: 0.5526387691497803\n",
      "epoch: 63     loss: 0.5544288754463196\n",
      "epoch: 64     loss: 0.5706027150154114\n",
      "epoch: 65     loss: 0.5526452660560608\n",
      "epoch: 66     loss: 0.5519812703132629\n",
      "epoch: 67     loss: 0.5504612922668457\n",
      "epoch: 68     loss: 0.5621082782745361\n",
      "epoch: 69     loss: 0.5547130107879639\n",
      "epoch: 70     loss: 0.551442563533783\n",
      "epoch: 71     loss: 0.5474850535392761\n",
      "epoch: 72     loss: 0.5478557348251343\n",
      "epoch: 73     loss: 0.547031044960022\n",
      "epoch: 74     loss: 0.5432658791542053\n",
      "epoch: 75     loss: 0.545735239982605\n",
      "epoch: 76     loss: 0.5449939966201782\n",
      "epoch: 77     loss: 0.5423665642738342\n",
      "epoch: 78     loss: 0.5427888631820679\n",
      "epoch: 79     loss: 0.5425400137901306\n",
      "epoch: 80     loss: 0.541239857673645\n",
      "epoch: 81     loss: 0.5375398993492126\n",
      "epoch: 82     loss: 0.5404466390609741\n",
      "epoch: 83     loss: 0.5365508198738098\n",
      "epoch: 84     loss: 0.5378455519676208\n",
      "epoch: 85     loss: 0.5384337306022644\n",
      "epoch: 86     loss: 0.5412959456443787\n",
      "epoch: 87     loss: 0.5378652811050415\n",
      "epoch: 88     loss: 0.5367618799209595\n",
      "epoch: 89     loss: 0.5328823328018188\n",
      "epoch: 90     loss: 0.5347586870193481\n",
      "epoch: 91     loss: 0.5320985317230225\n",
      "epoch: 92     loss: 0.5330607891082764\n",
      "epoch: 93     loss: 0.5311394333839417\n",
      "epoch: 94     loss: 0.5310408473014832\n",
      "epoch: 95     loss: 0.5299003720283508\n",
      "epoch: 96     loss: 0.5300171971321106\n",
      "epoch: 97     loss: 0.5277270674705505\n",
      "epoch: 98     loss: 0.5269681215286255\n",
      "epoch: 99     loss: 0.5255003571510315\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        x, y = HRdataset[i * batch: i * batch + batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用DataLoader重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch DataLoader负责管理批次，DataLoader从Dataset创建，自动为我们提供每个小批量，使遍历批次变得更容易，无需使用`HRdataset[i * batch: i * batch + batch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "HRdataloader = DataLoader(HRdataset, batch_size=batch, shuffle=True) # batch_size: 每个批次的大小为，shuffle: 是否打乱数据\n",
    "\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.562542736530304\n",
      "epoch: 1     loss: 0.5622386932373047\n",
      "epoch: 2     loss: 0.5582834482192993\n",
      "epoch: 3     loss: 0.5551632642745972\n",
      "epoch: 4     loss: 0.5556789040565491\n",
      "epoch: 5     loss: 0.5492849946022034\n",
      "epoch: 6     loss: 0.5447328090667725\n",
      "epoch: 7     loss: 0.5390941500663757\n",
      "epoch: 8     loss: 0.5324408411979675\n",
      "epoch: 9     loss: 0.5262114405632019\n",
      "epoch: 10     loss: 0.5192989706993103\n",
      "epoch: 11     loss: 0.5145075917243958\n",
      "epoch: 12     loss: 0.5264143943786621\n",
      "epoch: 13     loss: 0.4992421567440033\n",
      "epoch: 14     loss: 0.5028964281082153\n",
      "epoch: 15     loss: 0.4899069368839264\n",
      "epoch: 16     loss: 0.47703325748443604\n",
      "epoch: 17     loss: 0.4779425263404846\n",
      "epoch: 18     loss: 0.4637320637702942\n",
      "epoch: 19     loss: 0.4662896394729614\n",
      "epoch: 20     loss: 0.4514622986316681\n",
      "epoch: 21     loss: 0.44509318470954895\n",
      "epoch: 22     loss: 0.44492584466934204\n",
      "epoch: 23     loss: 0.4300081133842468\n",
      "epoch: 24     loss: 0.4242995083332062\n",
      "epoch: 25     loss: 0.4182853400707245\n",
      "epoch: 26     loss: 0.4123519957065582\n",
      "epoch: 27     loss: 0.4076663851737976\n",
      "epoch: 28     loss: 0.4022241234779358\n",
      "epoch: 29     loss: 0.3996424078941345\n",
      "epoch: 30     loss: 0.3928834795951843\n",
      "epoch: 31     loss: 0.38728398084640503\n",
      "epoch: 32     loss: 0.3826884627342224\n",
      "epoch: 33     loss: 0.38954785466194153\n",
      "epoch: 34     loss: 0.37472477555274963\n",
      "epoch: 35     loss: 0.37464651465415955\n",
      "epoch: 36     loss: 0.3678179979324341\n",
      "epoch: 37     loss: 0.36911195516586304\n",
      "epoch: 38     loss: 0.36816641688346863\n",
      "epoch: 39     loss: 0.3580423593521118\n",
      "epoch: 40     loss: 0.35481131076812744\n",
      "epoch: 41     loss: 0.3556184768676758\n",
      "epoch: 42     loss: 0.3488291800022125\n",
      "epoch: 43     loss: 0.34743690490722656\n",
      "epoch: 44     loss: 0.3437677323818207\n",
      "epoch: 45     loss: 0.34259194135665894\n",
      "epoch: 46     loss: 0.3390187621116638\n",
      "epoch: 47     loss: 0.3364337384700775\n",
      "epoch: 48     loss: 0.334445595741272\n",
      "epoch: 49     loss: 0.34276023507118225\n",
      "epoch: 50     loss: 0.3509140908718109\n",
      "epoch: 51     loss: 0.32984694838523865\n",
      "epoch: 52     loss: 0.3264176845550537\n",
      "epoch: 53     loss: 0.3244095742702484\n",
      "epoch: 54     loss: 0.3220178186893463\n",
      "epoch: 55     loss: 0.32045745849609375\n",
      "epoch: 56     loss: 0.32036587595939636\n",
      "epoch: 57     loss: 0.3165225684642792\n",
      "epoch: 58     loss: 0.32024824619293213\n",
      "epoch: 59     loss: 0.3167838752269745\n",
      "epoch: 60     loss: 0.3139383792877197\n",
      "epoch: 61     loss: 0.3161044120788574\n",
      "epoch: 62     loss: 0.3163576126098633\n",
      "epoch: 63     loss: 0.3095041513442993\n",
      "epoch: 64     loss: 0.3106510043144226\n",
      "epoch: 65     loss: 0.30824118852615356\n",
      "epoch: 66     loss: 0.3072504997253418\n",
      "epoch: 67     loss: 0.311381459236145\n",
      "epoch: 68     loss: 0.30120083689689636\n",
      "epoch: 69     loss: 0.2994900643825531\n",
      "epoch: 70     loss: 0.30051928758621216\n",
      "epoch: 71     loss: 0.2964872419834137\n",
      "epoch: 72     loss: 0.29640498757362366\n",
      "epoch: 73     loss: 0.2947714328765869\n",
      "epoch: 74     loss: 0.2973780035972595\n",
      "epoch: 75     loss: 0.2928029000759125\n",
      "epoch: 76     loss: 0.2928246259689331\n",
      "epoch: 77     loss: 0.2902865707874298\n",
      "epoch: 78     loss: 0.28834134340286255\n",
      "epoch: 79     loss: 0.29227977991104126\n",
      "epoch: 80     loss: 0.2860904037952423\n",
      "epoch: 81     loss: 0.28697630763053894\n",
      "epoch: 82     loss: 0.2860051691532135\n",
      "epoch: 83     loss: 0.28867965936660767\n",
      "epoch: 84     loss: 0.28241896629333496\n",
      "epoch: 85     loss: 0.2837528884410858\n",
      "epoch: 86     loss: 0.2799462676048279\n",
      "epoch: 87     loss: 0.28068360686302185\n",
      "epoch: 88     loss: 0.2777254581451416\n",
      "epoch: 89     loss: 0.27847450971603394\n",
      "epoch: 90     loss: 0.27577677369117737\n",
      "epoch: 91     loss: 0.27515509724617004\n",
      "epoch: 92     loss: 0.27366501092910767\n",
      "epoch: 93     loss: 0.2786222994327545\n",
      "epoch: 94     loss: 0.27286502718925476\n",
      "epoch: 95     loss: 0.2810059189796448\n",
      "epoch: 96     loss: 0.2719498872756958\n",
      "epoch: 97     loss: 0.2745082676410675\n",
      "epoch: 98     loss: 0.2771475911140442\n",
      "epoch: 99     loss: 0.267802357673645\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x, y in HRdataloader:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们只是试图建立一个合理的训练循环以用于我们的训练数据。实际上，您始终还应该具有一个验证集，以识别您是否过度拟合。\n",
    "\n",
    "训练数据的乱序（shuffle）对于防止批次与过度拟合之间的相关性很重要。另一方面，无论我们是否乱序验证集，验证损失都是相同的。由于shufle需要额外的开销，因此shuffle验证数据没有任何意义。\n",
    "\n",
    "我们将为验证集使用批大小，该批大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。我们利用这一优势来使用更大的批量，并更快地计算损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn -i https://pypi.douban.com/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X_data, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_y = torch.from_numpy(test_y).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(test_x, test_y)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义计算正确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = (out>0.5).type(torch.IntTensor)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()在训练之前调用代表训练模式\n",
    "\n",
    "model.eval() 推理之前进行调用代表推理模式\n",
    "\n",
    "不同的模式仅会在使用nn.BatchNorm2d ，nn.Dropout等层时以确保这些不同阶段的行为正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(20, 64)\n",
    "        self.lin_2 = nn.Linear(64, 64)\n",
    "        self.lin_3 = nn.Linear(64, 64)\n",
    "        self.lin_4 = nn.Linear(64, 1)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = self.lin_1(input)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_2(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_3(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean_train = np.mean([accuracy(model(xb), yb) for xb, yb in train_dl])\n",
    "            acc_mean_val = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        acc_train.append(acc_mean_train)\n",
    "        acc_val.append(acc_mean_val)\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean_train, acc_mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建fit（）和get_data（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在，我们获取数据加载器和拟合模型的整个过程可以在3行代码中运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
