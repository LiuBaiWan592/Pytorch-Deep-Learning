{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二分类问题-HR数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>part</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years   part  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./dataset/HR.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   part                   14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# 数据基本信息\n",
    "print(data.info())\n",
    "print(data.part.unique())\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>...</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  IT  RandD  \\\n",
       "0                   3              0     1                      0   0      0   \n",
       "1                   6              0     1                      0   0      0   \n",
       "2                   4              0     1                      0   0      0   \n",
       "3                   5              0     1                      0   0      0   \n",
       "4                   3              0     1                      0   0      0   \n",
       "\n",
       "   ...  hr  management  marketing  product_mng  sales  support  technical  \\\n",
       "0  ...   0           0          0            0      1        0          0   \n",
       "1  ...   0           0          0            0      1        0          0   \n",
       "2  ...   0           0          0            0      1        0          0   \n",
       "3  ...   0           0          0            0      1        0          0   \n",
       "4  ...   0           0          0            0      1        0          0   \n",
       "\n",
       "   high  low  medium  \n",
       "0     0    1       0  \n",
       "1     0    0       1  \n",
       "2     0    0       1  \n",
       "3     0    1       0  \n",
       "4     0    1       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 简单的数据分析\n",
    "# data.groupby(['salary', 'part']).size()\n",
    "# .get_dummies()方法可以将分类数据转换为one-hot编码\n",
    "data = data.join(pd.get_dummies(data.part).astype(int)).join(pd.get_dummies(data.salary).astype(int))\n",
    "# 删除原来的分类数据\n",
    "data.drop(columns=['part', 'salary'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   IT                     14999 non-null  int32  \n",
      " 9   RandD                  14999 non-null  int32  \n",
      " 10  accounting             14999 non-null  int32  \n",
      " 11  hr                     14999 non-null  int32  \n",
      " 12  management             14999 non-null  int32  \n",
      " 13  marketing              14999 non-null  int32  \n",
      " 14  product_mng            14999 non-null  int32  \n",
      " 15  sales                  14999 non-null  int32  \n",
      " 16  support                14999 non-null  int32  \n",
      " 17  technical              14999 non-null  int32  \n",
      " 18  high                   14999 non-null  int32  \n",
      " 19  low                    14999 non-null  int32  \n",
      " 20  medium                 14999 non-null  int32  \n",
      "dtypes: float64(2), int32(13), int64(6)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "0    11428\n",
      "1     3571\n",
      "Name: count, dtype: int64\n",
      "0.7619174611640777\n"
     ]
    }
   ],
   "source": [
    "# 查看离职率\n",
    "print(data.left.value_counts())\n",
    "# 全部预测为不离职\n",
    "print(data.left.value_counts()[0] / data.left.value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 1])\n"
     ]
    }
   ],
   "source": [
    "# 处理结果数据\n",
    "Y_data = data.left.values.reshape(-1, 1)\n",
    "Y = torch.from_numpy(Y_data).type(torch.FloatTensor)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 20])\n"
     ]
    }
   ],
   "source": [
    "# 处理特征数据\n",
    "# 使用列表推导式，获取除了'left'列之外的所有列\n",
    "# [c for c in data.columns if c != 'left']\n",
    "# 使用.values方法，将DataFrame转换为numpy数组\n",
    "X_data = data[[c for c in data.columns if c != 'left']].values\n",
    "X = torch.from_numpy(X_data).type(torch.FloatTensor)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# # 自定义模型：逻辑回归模型\n",
    "# class Logistic(nn.Module):  # 继承nn.Module\n",
    "#     def __init__(self):     # 初始化所有的层\n",
    "#         super().__init__()  # 继承父类中所有的属性和方法\n",
    "#         self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "#         self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "#         self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "#         self.activate = nn.ReLU()       # 定义ReLU激活函数\n",
    "#         self.sigmoid = nn.Sigmoid()     # 定义Sigmoid激活函数\n",
    "#     def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "#         x = self.lin_1(input)   # 将输入数据传入第一层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_2(x)       # 将激活后的数据传入第二层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_3(x)       # 将激活后的数据传入第三层线性层\n",
    "#         x = self.sigmoid(x)     # Sigmoid激活函数\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型改写\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # 函数式API，调用方便使代码更简洁\n",
    "class Logistic(nn.Module):  # 继承nn.Module\n",
    "    def __init__(self):     # 初始化所有的层\n",
    "        super().__init__()  # 继承父类中所有的属性和方法\n",
    "        self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "        self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "        self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "    def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "        x = F.relu(self.lin_1(input))   # 将输入数据传入第一层线性层，并使用ReLU激活函数\n",
    "        x = F.relu(self.lin_2(x))       # 将激活后的数据传入第二层线性层，并使用ReLU激活函数\n",
    "        x = F.sigmoid(self.lin_3(x))     # 将激活后的数据传入第三层线性层，并使用Sigmoid激活函数\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Logistic(\n",
      "  (lin_1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (lin_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_3): Linear(in_features=64, out_features=1, bias=True)\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# 封装模型和优化器的创建，提高代码复用性\n",
    "lr = 0.0001\n",
    "def get_model():\n",
    "    model = Logistic()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, opt\n",
    "print(get_model())\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集，分批次进行训练\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 手动分批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6453866958618164\n",
      "epoch: 1     loss: 0.6687933206558228\n",
      "epoch: 2     loss: 0.676135778427124\n",
      "epoch: 3     loss: 0.6733677983283997\n",
      "epoch: 4     loss: 0.6705600619316101\n",
      "epoch: 5     loss: 0.6662737131118774\n",
      "epoch: 6     loss: 0.660861074924469\n",
      "epoch: 7     loss: 0.6547017693519592\n",
      "epoch: 8     loss: 0.6480571627616882\n",
      "epoch: 9     loss: 0.6411466598510742\n",
      "epoch: 10     loss: 0.6344260573387146\n",
      "epoch: 11     loss: 0.6272299289703369\n",
      "epoch: 12     loss: 0.6201618313789368\n",
      "epoch: 13     loss: 0.6132879853248596\n",
      "epoch: 14     loss: 0.6066782474517822\n",
      "epoch: 15     loss: 0.6003884077072144\n",
      "epoch: 16     loss: 0.594443678855896\n",
      "epoch: 17     loss: 0.5888413786888123\n",
      "epoch: 18     loss: 0.575970470905304\n",
      "epoch: 19     loss: 0.5913407206535339\n",
      "epoch: 20     loss: 0.5849275588989258\n",
      "epoch: 21     loss: 0.5791508555412292\n",
      "epoch: 22     loss: 0.5747002959251404\n",
      "epoch: 23     loss: 0.5709288120269775\n",
      "epoch: 24     loss: 0.5676838159561157\n",
      "epoch: 25     loss: 0.564943253993988\n",
      "epoch: 26     loss: 0.5626326203346252\n",
      "epoch: 27     loss: 0.5607658624649048\n",
      "epoch: 28     loss: 0.5591959357261658\n",
      "epoch: 29     loss: 0.5580360889434814\n",
      "epoch: 30     loss: 0.5572351813316345\n",
      "epoch: 31     loss: 0.5562650561332703\n",
      "epoch: 32     loss: 0.5603799223899841\n",
      "epoch: 33     loss: 0.5577492117881775\n",
      "epoch: 34     loss: 0.5565148591995239\n",
      "epoch: 35     loss: 0.5556607246398926\n",
      "epoch: 36     loss: 0.5577234625816345\n",
      "epoch: 37     loss: 0.5577163696289062\n",
      "epoch: 38     loss: 0.554869532585144\n",
      "epoch: 39     loss: 0.5541983842849731\n",
      "epoch: 40     loss: 0.553457498550415\n",
      "epoch: 41     loss: 0.5533044934272766\n",
      "epoch: 42     loss: 0.5530802607536316\n",
      "epoch: 43     loss: 0.5544841289520264\n",
      "epoch: 44     loss: 0.5551419854164124\n",
      "epoch: 45     loss: 0.5531582832336426\n",
      "epoch: 46     loss: 0.5526586174964905\n",
      "epoch: 47     loss: 0.552879810333252\n",
      "epoch: 48     loss: 0.5525048971176147\n",
      "epoch: 49     loss: 0.5529161691665649\n",
      "epoch: 50     loss: 0.5620615482330322\n",
      "epoch: 51     loss: 0.5527584552764893\n",
      "epoch: 52     loss: 0.5525452494621277\n",
      "epoch: 53     loss: 0.5537357926368713\n",
      "epoch: 54     loss: 0.5531052350997925\n",
      "epoch: 55     loss: 0.5523725152015686\n",
      "epoch: 56     loss: 0.5537311434745789\n",
      "epoch: 57     loss: 0.5522193312644958\n",
      "epoch: 58     loss: 0.5546085238456726\n",
      "epoch: 59     loss: 0.5513356328010559\n",
      "epoch: 60     loss: 0.5540266036987305\n",
      "epoch: 61     loss: 0.5519199967384338\n",
      "epoch: 62     loss: 0.5536879301071167\n",
      "epoch: 63     loss: 0.5511976480484009\n",
      "epoch: 64     loss: 0.5526451468467712\n",
      "epoch: 65     loss: 0.5556153059005737\n",
      "epoch: 66     loss: 0.5562295317649841\n",
      "epoch: 67     loss: 0.5540362596511841\n",
      "epoch: 68     loss: 0.5510933995246887\n",
      "epoch: 69     loss: 0.5541060566902161\n",
      "epoch: 70     loss: 0.553912341594696\n",
      "epoch: 71     loss: 0.5489748120307922\n",
      "epoch: 72     loss: 0.5532151460647583\n",
      "epoch: 73     loss: 0.5482991933822632\n",
      "epoch: 74     loss: 0.5781162977218628\n",
      "epoch: 75     loss: 0.5508995652198792\n",
      "epoch: 76     loss: 0.5517763495445251\n",
      "epoch: 77     loss: 0.5479093194007874\n",
      "epoch: 78     loss: 0.5504011511802673\n",
      "epoch: 79     loss: 0.5467342138290405\n",
      "epoch: 80     loss: 0.5487130284309387\n",
      "epoch: 81     loss: 0.5457656979560852\n",
      "epoch: 82     loss: 0.5480461716651917\n",
      "epoch: 83     loss: 0.5446330904960632\n",
      "epoch: 84     loss: 0.5610411167144775\n",
      "epoch: 85     loss: 0.5474686622619629\n",
      "epoch: 86     loss: 0.5466495156288147\n",
      "epoch: 87     loss: 0.5423817038536072\n",
      "epoch: 88     loss: 0.5441451072692871\n",
      "epoch: 89     loss: 0.5410497188568115\n",
      "epoch: 90     loss: 0.5418345332145691\n",
      "epoch: 91     loss: 0.5400620102882385\n",
      "epoch: 92     loss: 0.5400017499923706\n",
      "epoch: 93     loss: 0.5387496948242188\n",
      "epoch: 94     loss: 0.5386174917221069\n",
      "epoch: 95     loss: 0.5373446941375732\n",
      "epoch: 96     loss: 0.5363138914108276\n",
      "epoch: 97     loss: 0.535971999168396\n",
      "epoch: 98     loss: 0.5355817079544067\n",
      "epoch: 99     loss: 0.5358924269676208\n"
     ]
    }
   ],
   "source": [
    "# 分批次循环训练\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):     # 按照批次进行训练\n",
    "        start = i * batch              # 每个批次的起始索引\n",
    "        end = start + batch            # 每个批次的结束索引\n",
    "        x = X[start: end]\n",
    "        y = Y[start: end]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute loss: BCELoss expects the target to be between 0 and 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Gradient reset\n",
    "        opt.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用dataset重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch有一个抽象的Dataset类。Dataset可以是任何具有__len__函数和__getitem__作为对其进行索引的方法的函数。PyTorch的TensorDataset是一个包装张量的Dataset。通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。这将使我们在训练的同一行中更容易访问自变量和因变量。下面将自定义HRDataset类创建为的Dataset的子类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "# print(HRdataset[2: 5])\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.5720505118370056\n",
      "epoch: 1     loss: 0.567107617855072\n",
      "epoch: 2     loss: 0.5683744549751282\n",
      "epoch: 3     loss: 0.5662668347358704\n",
      "epoch: 4     loss: 0.5654147863388062\n",
      "epoch: 5     loss: 0.5655049681663513\n",
      "epoch: 6     loss: 0.5657234191894531\n",
      "epoch: 7     loss: 0.5646464228630066\n",
      "epoch: 8     loss: 0.5655176639556885\n",
      "epoch: 9     loss: 0.5637344121932983\n",
      "epoch: 10     loss: 0.5647470355033875\n",
      "epoch: 11     loss: 0.5646924376487732\n",
      "epoch: 12     loss: 0.5656791925430298\n",
      "epoch: 13     loss: 0.5660978555679321\n",
      "epoch: 14     loss: 0.5651789307594299\n",
      "epoch: 15     loss: 0.5664445757865906\n",
      "epoch: 16     loss: 0.5665634274482727\n",
      "epoch: 17     loss: 0.5668529868125916\n",
      "epoch: 18     loss: 0.5668807625770569\n",
      "epoch: 19     loss: 0.566774845123291\n",
      "epoch: 20     loss: 0.5665484666824341\n",
      "epoch: 21     loss: 0.5664899945259094\n",
      "epoch: 22     loss: 0.5666607022285461\n",
      "epoch: 23     loss: 0.5663838982582092\n",
      "epoch: 24     loss: 0.56585294008255\n",
      "epoch: 25     loss: 0.5708792805671692\n",
      "epoch: 26     loss: 0.566480278968811\n",
      "epoch: 27     loss: 0.5742501616477966\n",
      "epoch: 28     loss: 0.5681464672088623\n",
      "epoch: 29     loss: 0.5800063014030457\n",
      "epoch: 30     loss: 0.5645868182182312\n",
      "epoch: 31     loss: 0.5655014514923096\n",
      "epoch: 32     loss: 0.5653716921806335\n",
      "epoch: 33     loss: 0.565818190574646\n",
      "epoch: 34     loss: 0.5653815865516663\n",
      "epoch: 35     loss: 0.5655047297477722\n",
      "epoch: 36     loss: 0.5649594664573669\n",
      "epoch: 37     loss: 0.5731992125511169\n",
      "epoch: 38     loss: 0.5643959641456604\n",
      "epoch: 39     loss: 0.5648240447044373\n",
      "epoch: 40     loss: 0.5632469654083252\n",
      "epoch: 41     loss: 0.5735311508178711\n",
      "epoch: 42     loss: 0.5668196678161621\n",
      "epoch: 43     loss: 0.5664874911308289\n",
      "epoch: 44     loss: 0.5654672384262085\n",
      "epoch: 45     loss: 0.5897917151451111\n",
      "epoch: 46     loss: 0.5682573318481445\n",
      "epoch: 47     loss: 0.5643789172172546\n",
      "epoch: 48     loss: 0.5662023425102234\n",
      "epoch: 49     loss: 0.5647084712982178\n",
      "epoch: 50     loss: 0.564275324344635\n",
      "epoch: 51     loss: 0.5719478130340576\n",
      "epoch: 52     loss: 0.5646097660064697\n",
      "epoch: 53     loss: 0.5630932450294495\n",
      "epoch: 54     loss: 0.5634763836860657\n",
      "epoch: 55     loss: 0.5626890659332275\n",
      "epoch: 56     loss: 0.562857449054718\n",
      "epoch: 57     loss: 0.5622100234031677\n",
      "epoch: 58     loss: 0.5616582036018372\n",
      "epoch: 59     loss: 0.5613841414451599\n",
      "epoch: 60     loss: 0.5625659823417664\n",
      "epoch: 61     loss: 0.5597996115684509\n",
      "epoch: 62     loss: 0.559544026851654\n",
      "epoch: 63     loss: 0.5589334964752197\n",
      "epoch: 64     loss: 0.5906038284301758\n",
      "epoch: 65     loss: 0.587569534778595\n",
      "epoch: 66     loss: 0.5630838871002197\n",
      "epoch: 67     loss: 0.5609230995178223\n",
      "epoch: 68     loss: 0.5597533583641052\n",
      "epoch: 69     loss: 0.5585148334503174\n",
      "epoch: 70     loss: 0.5584914088249207\n",
      "epoch: 71     loss: 0.5566704273223877\n",
      "epoch: 72     loss: 0.5577600598335266\n",
      "epoch: 73     loss: 0.5568298697471619\n",
      "epoch: 74     loss: 0.5566076636314392\n",
      "epoch: 75     loss: 0.5551527142524719\n",
      "epoch: 76     loss: 0.5543410778045654\n",
      "epoch: 77     loss: 0.5529006123542786\n",
      "epoch: 78     loss: 0.554676353931427\n",
      "epoch: 79     loss: 0.5528082251548767\n",
      "epoch: 80     loss: 0.5505934953689575\n",
      "epoch: 81     loss: 0.5649064779281616\n",
      "epoch: 82     loss: 0.6153379082679749\n",
      "epoch: 83     loss: 0.5578283071517944\n",
      "epoch: 84     loss: 0.5526772737503052\n",
      "epoch: 85     loss: 0.5675556063652039\n",
      "epoch: 86     loss: 0.5479699373245239\n",
      "epoch: 87     loss: 0.5543796420097351\n",
      "epoch: 88     loss: 0.5424304604530334\n",
      "epoch: 89     loss: 0.5378580689430237\n",
      "epoch: 90     loss: 0.5391013622283936\n",
      "epoch: 91     loss: 0.5360767841339111\n",
      "epoch: 92     loss: 0.5358931422233582\n",
      "epoch: 93     loss: 0.5351557731628418\n",
      "epoch: 94     loss: 0.5342608094215393\n",
      "epoch: 95     loss: 0.5327646732330322\n",
      "epoch: 96     loss: 0.5317334532737732\n",
      "epoch: 97     loss: 0.5320895314216614\n",
      "epoch: 98     loss: 0.5311657786369324\n",
      "epoch: 99     loss: 0.527666449546814\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        x, y = HRdataset[i * batch: i * batch + batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用DataLoader重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch DataLoader负责管理批次，DataLoader从Dataset创建，自动为我们提供每个小批量，使遍历批次变得更容易，无需使用`HRdataset[i * batch: i * batch + batch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "HRdataloader = DataLoader(HRdataset, batch_size=batch) # batch_size: 每个批次的大小为，shuffle: 是否打乱数据\n",
    "\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.7572331428527832\n",
      "epoch: 1     loss: 0.8513616323471069\n",
      "epoch: 2     loss: 0.7825905084609985\n",
      "epoch: 3     loss: 0.7682247757911682\n",
      "epoch: 4     loss: 0.7508987784385681\n",
      "epoch: 5     loss: 0.7392633557319641\n",
      "epoch: 6     loss: 0.7274201512336731\n",
      "epoch: 7     loss: 0.7154132127761841\n",
      "epoch: 8     loss: 0.7033049464225769\n",
      "epoch: 9     loss: 0.6913676261901855\n",
      "epoch: 10     loss: 0.6793896555900574\n",
      "epoch: 11     loss: 0.6677554249763489\n",
      "epoch: 12     loss: 0.6562513709068298\n",
      "epoch: 13     loss: 0.6451342701911926\n",
      "epoch: 14     loss: 0.681895911693573\n",
      "epoch: 15     loss: 0.6255367994308472\n",
      "epoch: 16     loss: 0.6153163313865662\n",
      "epoch: 17     loss: 0.6069347858428955\n",
      "epoch: 18     loss: 0.6004155874252319\n",
      "epoch: 19     loss: 0.5885879397392273\n",
      "epoch: 20     loss: 0.5815191268920898\n",
      "epoch: 21     loss: 0.5793758630752563\n",
      "epoch: 22     loss: 0.5713782906532288\n",
      "epoch: 23     loss: 0.5672673583030701\n",
      "epoch: 24     loss: 0.5619721412658691\n",
      "epoch: 25     loss: 0.5598342418670654\n",
      "epoch: 26     loss: 0.5660703182220459\n",
      "epoch: 27     loss: 0.5571390986442566\n",
      "epoch: 28     loss: 0.5577812790870667\n",
      "epoch: 29     loss: 0.5573499202728271\n",
      "epoch: 30     loss: 0.5561898350715637\n",
      "epoch: 31     loss: 0.5553949475288391\n",
      "epoch: 32     loss: 0.5551375150680542\n",
      "epoch: 33     loss: 0.5565922856330872\n",
      "epoch: 34     loss: 0.5586398243904114\n",
      "epoch: 35     loss: 0.5568647384643555\n",
      "epoch: 36     loss: 0.5553521513938904\n",
      "epoch: 37     loss: 0.5554739832878113\n",
      "epoch: 38     loss: 0.5555538535118103\n",
      "epoch: 39     loss: 0.5555519461631775\n",
      "epoch: 40     loss: 0.555607795715332\n",
      "epoch: 41     loss: 0.5557425022125244\n",
      "epoch: 42     loss: 0.5646477341651917\n",
      "epoch: 43     loss: 0.5536855459213257\n",
      "epoch: 44     loss: 0.5593445897102356\n",
      "epoch: 45     loss: 0.5544379353523254\n",
      "epoch: 46     loss: 0.5580791234970093\n",
      "epoch: 47     loss: 0.5564790368080139\n",
      "epoch: 48     loss: 0.5592715740203857\n",
      "epoch: 49     loss: 0.5569701790809631\n",
      "epoch: 50     loss: 0.5554965138435364\n",
      "epoch: 51     loss: 0.5582171082496643\n",
      "epoch: 52     loss: 0.5552542805671692\n",
      "epoch: 53     loss: 0.562690794467926\n",
      "epoch: 54     loss: 0.5546936988830566\n",
      "epoch: 55     loss: 0.5539600849151611\n",
      "epoch: 56     loss: 0.5543438792228699\n",
      "epoch: 57     loss: 0.5556237697601318\n",
      "epoch: 58     loss: 0.548923909664154\n",
      "epoch: 59     loss: 0.5608880519866943\n",
      "epoch: 60     loss: 0.5508190393447876\n",
      "epoch: 61     loss: 0.5548872351646423\n",
      "epoch: 62     loss: 0.5522500276565552\n",
      "epoch: 63     loss: 0.5535224676132202\n",
      "epoch: 64     loss: 0.5513154864311218\n",
      "epoch: 65     loss: 0.5512441992759705\n",
      "epoch: 66     loss: 0.5501912236213684\n",
      "epoch: 67     loss: 0.5494144558906555\n",
      "epoch: 68     loss: 0.5506274104118347\n",
      "epoch: 69     loss: 0.5510116219520569\n",
      "epoch: 70     loss: 0.5472871661186218\n",
      "epoch: 71     loss: 0.5490320324897766\n",
      "epoch: 72     loss: 0.5450423955917358\n",
      "epoch: 73     loss: 0.5490897297859192\n",
      "epoch: 74     loss: 0.5447248816490173\n",
      "epoch: 75     loss: 0.5549249053001404\n",
      "epoch: 76     loss: 0.5452125668525696\n",
      "epoch: 77     loss: 0.5440555810928345\n",
      "epoch: 78     loss: 0.5589950680732727\n",
      "epoch: 79     loss: 0.5446813106536865\n",
      "epoch: 80     loss: 0.5477310419082642\n",
      "epoch: 81     loss: 0.5433820486068726\n",
      "epoch: 82     loss: 0.5445117950439453\n",
      "epoch: 83     loss: 0.5444357395172119\n",
      "epoch: 84     loss: 0.5417070388793945\n",
      "epoch: 85     loss: 0.5398969054222107\n",
      "epoch: 86     loss: 0.5430145859718323\n",
      "epoch: 87     loss: 0.54353928565979\n",
      "epoch: 88     loss: 0.5439685583114624\n",
      "epoch: 89     loss: 0.5410116314888\n",
      "epoch: 90     loss: 0.5415542125701904\n",
      "epoch: 91     loss: 0.5661322474479675\n",
      "epoch: 92     loss: 0.5434266328811646\n",
      "epoch: 93     loss: 0.5389766693115234\n",
      "epoch: 94     loss: 0.5362311005592346\n",
      "epoch: 95     loss: 0.5334958434104919\n",
      "epoch: 96     loss: 0.5339294075965881\n",
      "epoch: 97     loss: 0.5313003659248352\n",
      "epoch: 98     loss: 0.5305524468421936\n",
      "epoch: 99     loss: 0.5299815535545349\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x, y in HRdataloader:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分验证数据集和测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**过拟合：** 指模型在训练数据上表现良好，但在验证数据（未知数据）上表现不佳。  \n",
    "**欠拟合：** 指模型在训练数据上表现不佳，在验证数据上表现不佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们只是试图建立一个合理的训练循环以用于我们的训练数据。实际上，始终还应该具有一个验证集，以识别是否过度拟合。\n",
    "\n",
    "训练数据的乱序（shuffle）对于防止批次与过度拟合之间的相关性很重要。另一方面，无论我们是否乱序验证集，验证损失都是相同的。由于shufle需要额外的开销，因此shuffle验证数据没有任何意义。\n",
    "\n",
    "我们将为验证集使用批大小，该批大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。我们利用这一优势来使用更大的批量，并更快地计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11249, 20) (11249, 1) (3750, 20) (3750, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_data, Y_data) # 划分训练集和测试集\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将numpy数组转换为PyTorch张量\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_y = torch.from_numpy(test_y).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PyTorch的TensorDataset和DataLoader将数据集转换为数据加载器\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch)\n",
    "\n",
    "valid_ds = TensorDataset(test_x, test_y)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义计算正确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = (out>0.5).type(torch.IntTensor)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()在训练之前调用代表训练模式\n",
    "\n",
    "model.eval() 推理之前进行调用代表推理模式\n",
    "\n",
    "不同的模式仅会在使用nn.BatchNorm2d ，nn.Dropout等层时以确保这些不同阶段的行为正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(20, 64)\n",
    "        self.lin_2 = nn.Linear(64, 64)\n",
    "        self.lin_3 = nn.Linear(64, 64)\n",
    "        self.lin_4 = nn.Linear(64, 1)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = self.lin_1(input)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_2(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_3(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean_train = np.mean([accuracy(model(xb), yb) for xb, yb in train_dl])\n",
    "            acc_mean_val = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        acc_train.append(acc_mean_train)\n",
    "        acc_val.append(acc_mean_val)\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean_train, acc_mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建fit（）和get_data（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在，我们获取数据加载器和拟合模型的整个过程可以在3行代码中运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
