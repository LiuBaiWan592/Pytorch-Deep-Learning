{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二分类问题-HR数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>part</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years   part  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./dataset/HR.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   part                   14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# 数据基本信息\n",
    "print(data.info())\n",
    "print(data.part.unique())\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>...</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  IT  RandD  \\\n",
       "0                   3              0     1                      0   0      0   \n",
       "1                   6              0     1                      0   0      0   \n",
       "2                   4              0     1                      0   0      0   \n",
       "3                   5              0     1                      0   0      0   \n",
       "4                   3              0     1                      0   0      0   \n",
       "\n",
       "   ...  hr  management  marketing  product_mng  sales  support  technical  \\\n",
       "0  ...   0           0          0            0      1        0          0   \n",
       "1  ...   0           0          0            0      1        0          0   \n",
       "2  ...   0           0          0            0      1        0          0   \n",
       "3  ...   0           0          0            0      1        0          0   \n",
       "4  ...   0           0          0            0      1        0          0   \n",
       "\n",
       "   high  low  medium  \n",
       "0     0    1       0  \n",
       "1     0    0       1  \n",
       "2     0    0       1  \n",
       "3     0    1       0  \n",
       "4     0    1       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 简单的数据分析\n",
    "# data.groupby(['salary', 'part']).size()\n",
    "# .get_dummies()方法可以将分类数据转换为one-hot编码\n",
    "data = data.join(pd.get_dummies(data.part).astype(int)).join(pd.get_dummies(data.salary).astype(int))\n",
    "# 删除原来的分类数据\n",
    "data.drop(columns=['part', 'salary'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   IT                     14999 non-null  int32  \n",
      " 9   RandD                  14999 non-null  int32  \n",
      " 10  accounting             14999 non-null  int32  \n",
      " 11  hr                     14999 non-null  int32  \n",
      " 12  management             14999 non-null  int32  \n",
      " 13  marketing              14999 non-null  int32  \n",
      " 14  product_mng            14999 non-null  int32  \n",
      " 15  sales                  14999 non-null  int32  \n",
      " 16  support                14999 non-null  int32  \n",
      " 17  technical              14999 non-null  int32  \n",
      " 18  high                   14999 non-null  int32  \n",
      " 19  low                    14999 non-null  int32  \n",
      " 20  medium                 14999 non-null  int32  \n",
      "dtypes: float64(2), int32(13), int64(6)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "0    11428\n",
      "1     3571\n",
      "Name: count, dtype: int64\n",
      "0.7619174611640777\n"
     ]
    }
   ],
   "source": [
    "# 查看离职率\n",
    "print(data.left.value_counts())\n",
    "# 全部预测为不离职\n",
    "print(data.left.value_counts()[0] / data.left.value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 1])\n"
     ]
    }
   ],
   "source": [
    "# 处理结果数据\n",
    "Y_data = data.left.values.reshape(-1, 1)\n",
    "Y = torch.from_numpy(Y_data).type(torch.FloatTensor)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 20])\n"
     ]
    }
   ],
   "source": [
    "# 处理特征数据\n",
    "# 使用列表推导式，获取除了'left'列之外的所有列\n",
    "# [c for c in data.columns if c != 'left']\n",
    "# 使用.values方法，将DataFrame转换为numpy数组\n",
    "X_data = data[[c for c in data.columns if c != 'left']].values\n",
    "X = torch.from_numpy(X_data).type(torch.FloatTensor)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# # 自定义模型：逻辑回归模型\n",
    "# class Logistic(nn.Module):  # 继承nn.Module\n",
    "#     def __init__(self):     # 初始化所有的层\n",
    "#         super().__init__()  # 继承父类中所有的属性和方法\n",
    "#         self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "#         self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "#         self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "#         self.activate = nn.ReLU()       # 定义ReLU激活函数\n",
    "#         self.sigmoid = nn.Sigmoid()     # 定义Sigmoid激活函数\n",
    "#     def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "#         x = self.lin_1(input)   # 将输入数据传入第一层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_2(x)       # 将激活后的数据传入第二层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_3(x)       # 将激活后的数据传入第三层线性层\n",
    "#         x = self.sigmoid(x)     # Sigmoid激活函数\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型改写\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # 函数式API，调用方便使代码更简洁\n",
    "class Logistic(nn.Module):  # 继承nn.Module\n",
    "    def __init__(self):     # 初始化所有的层\n",
    "        super().__init__()  # 继承父类中所有的属性和方法\n",
    "        self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "        self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "        self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "    def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "        x = F.relu(self.lin_1(input))   # 将输入数据传入第一层线性层，并使用ReLU激活函数\n",
    "        x = F.relu(self.lin_2(x))       # 将激活后的数据传入第二层线性层，并使用ReLU激活函数\n",
    "        x = F.sigmoid(self.lin_3(x))     # 将激活后的数据传入第三层线性层，并使用Sigmoid激活函数\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Logistic(\n",
      "  (lin_1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (lin_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_3): Linear(in_features=64, out_features=1, bias=True)\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# 封装模型和优化器的创建，提高代码复用性\n",
    "lr = 0.0001\n",
    "def get_model():\n",
    "    model = Logistic()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, opt\n",
    "print(get_model())\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集，分批次进行训练\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 手动分批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6653244495391846\n",
      "epoch: 1     loss: 0.6724502444267273\n",
      "epoch: 2     loss: 0.66326904296875\n",
      "epoch: 3     loss: 0.6563212871551514\n",
      "epoch: 4     loss: 0.6486781239509583\n",
      "epoch: 5     loss: 0.6963856816291809\n",
      "epoch: 6     loss: 0.6340402364730835\n",
      "epoch: 7     loss: 0.6262431144714355\n",
      "epoch: 8     loss: 0.6186566352844238\n",
      "epoch: 9     loss: 0.610680878162384\n",
      "epoch: 10     loss: 0.624306857585907\n",
      "epoch: 11     loss: 0.5956229567527771\n",
      "epoch: 12     loss: 0.5905640721321106\n",
      "epoch: 13     loss: 0.5859825611114502\n",
      "epoch: 14     loss: 0.5818182826042175\n",
      "epoch: 15     loss: 0.578037440776825\n",
      "epoch: 16     loss: 0.5848634839057922\n",
      "epoch: 17     loss: 0.5720465779304504\n",
      "epoch: 18     loss: 0.5694150328636169\n",
      "epoch: 19     loss: 0.5671862363815308\n",
      "epoch: 20     loss: 0.5652644038200378\n",
      "epoch: 21     loss: 0.5636811852455139\n",
      "epoch: 22     loss: 0.5623966455459595\n",
      "epoch: 23     loss: 0.5613612532615662\n",
      "epoch: 24     loss: 0.580359935760498\n",
      "epoch: 25     loss: 0.5601328015327454\n",
      "epoch: 26     loss: 0.5598888397216797\n",
      "epoch: 27     loss: 0.5596734881401062\n",
      "epoch: 28     loss: 0.5596132278442383\n",
      "epoch: 29     loss: 0.5596141219139099\n",
      "epoch: 30     loss: 0.559776782989502\n",
      "epoch: 31     loss: 0.5616737008094788\n",
      "epoch: 32     loss: 0.5591868758201599\n",
      "epoch: 33     loss: 0.5598408579826355\n",
      "epoch: 34     loss: 0.5615217685699463\n",
      "epoch: 35     loss: 0.5597636699676514\n",
      "epoch: 36     loss: 0.5620923042297363\n",
      "epoch: 37     loss: 0.5620797872543335\n",
      "epoch: 38     loss: 0.5608329772949219\n",
      "epoch: 39     loss: 0.5618199706077576\n",
      "epoch: 40     loss: 0.5620315074920654\n",
      "epoch: 41     loss: 0.5693978667259216\n",
      "epoch: 42     loss: 0.5613442063331604\n",
      "epoch: 43     loss: 0.5621647834777832\n",
      "epoch: 44     loss: 0.5621275901794434\n",
      "epoch: 45     loss: 0.5622699856758118\n",
      "epoch: 46     loss: 0.5628376603126526\n",
      "epoch: 47     loss: 0.5615686178207397\n",
      "epoch: 48     loss: 0.5624603033065796\n",
      "epoch: 49     loss: 0.5613117218017578\n",
      "epoch: 50     loss: 0.5620439052581787\n",
      "epoch: 51     loss: 0.5644301772117615\n",
      "epoch: 52     loss: 0.558138370513916\n",
      "epoch: 53     loss: 0.5606213808059692\n",
      "epoch: 54     loss: 0.5606685876846313\n",
      "epoch: 55     loss: 0.56052166223526\n",
      "epoch: 56     loss: 0.5652745962142944\n",
      "epoch: 57     loss: 0.5590802431106567\n",
      "epoch: 58     loss: 0.560001790523529\n",
      "epoch: 59     loss: 0.5599151849746704\n",
      "epoch: 60     loss: 0.5596983432769775\n",
      "epoch: 61     loss: 0.5594485998153687\n",
      "epoch: 62     loss: 0.5591833591461182\n",
      "epoch: 63     loss: 0.5589162707328796\n",
      "epoch: 64     loss: 0.558548092842102\n",
      "epoch: 65     loss: 0.5560385584831238\n",
      "epoch: 66     loss: 0.5577181577682495\n",
      "epoch: 67     loss: 0.5578632354736328\n",
      "epoch: 68     loss: 0.5576692223548889\n",
      "epoch: 69     loss: 0.5602340698242188\n",
      "epoch: 70     loss: 0.5588377714157104\n",
      "epoch: 71     loss: 0.5571046471595764\n",
      "epoch: 72     loss: 0.5568392276763916\n",
      "epoch: 73     loss: 0.556615948677063\n",
      "epoch: 74     loss: 0.5563758015632629\n",
      "epoch: 75     loss: 0.5561412572860718\n",
      "epoch: 76     loss: 0.5559143424034119\n",
      "epoch: 77     loss: 0.5556930899620056\n",
      "epoch: 78     loss: 0.5552341341972351\n",
      "epoch: 79     loss: 0.5728468298912048\n",
      "epoch: 80     loss: 0.5561484694480896\n",
      "epoch: 81     loss: 0.554599404335022\n",
      "epoch: 82     loss: 0.5539093017578125\n",
      "epoch: 83     loss: 0.5534027218818665\n",
      "epoch: 84     loss: 0.5530707240104675\n",
      "epoch: 85     loss: 0.5526936650276184\n",
      "epoch: 86     loss: 0.5523322224617004\n",
      "epoch: 87     loss: 0.5520640015602112\n",
      "epoch: 88     loss: 0.5540891885757446\n",
      "epoch: 89     loss: 0.5527768731117249\n",
      "epoch: 90     loss: 0.5522754788398743\n",
      "epoch: 91     loss: 0.5517933368682861\n",
      "epoch: 92     loss: 0.5529729127883911\n",
      "epoch: 93     loss: 0.5509827136993408\n",
      "epoch: 94     loss: 0.5506672859191895\n",
      "epoch: 95     loss: 0.5504059195518494\n",
      "epoch: 96     loss: 0.5502349734306335\n",
      "epoch: 97     loss: 0.5498477220535278\n",
      "epoch: 98     loss: 0.5494824647903442\n",
      "epoch: 99     loss: 0.5490240454673767\n"
     ]
    }
   ],
   "source": [
    "# 分批次循环训练\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):     # 按照批次进行训练\n",
    "        start = i * batch              # 每个批次的起始索引\n",
    "        end = start + batch            # 每个批次的结束索引\n",
    "        x = X[start: end]\n",
    "        y = Y[start: end]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute loss: BCELoss expects the target to be between 0 and 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Gradient reset\n",
    "        opt.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用dataset重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch有一个抽象的Dataset类。Dataset可以是任何具有__len__函数和__getitem__作为对其进行索引的方法的函数。PyTorch的TensorDataset是一个包装张量的Dataset。通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。这将使我们在训练的同一行中更容易访问自变量和因变量。下面将自定义HRDataset类创建为的Dataset的子类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "# print(HRdataset[2: 5])\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6955068111419678\n",
      "epoch: 1     loss: 0.6839310526847839\n",
      "epoch: 2     loss: 0.6755874752998352\n",
      "epoch: 3     loss: 0.6657107472419739\n",
      "epoch: 4     loss: 0.6551401615142822\n",
      "epoch: 5     loss: 0.6444115042686462\n",
      "epoch: 6     loss: 0.6337931752204895\n",
      "epoch: 7     loss: 0.6235368847846985\n",
      "epoch: 8     loss: 0.6138008236885071\n",
      "epoch: 9     loss: 0.618353009223938\n",
      "epoch: 10     loss: 0.6162864565849304\n",
      "epoch: 11     loss: 0.6000204682350159\n",
      "epoch: 12     loss: 0.5915645956993103\n",
      "epoch: 13     loss: 0.5849820375442505\n",
      "epoch: 14     loss: 0.5794337391853333\n",
      "epoch: 15     loss: 0.5755189657211304\n",
      "epoch: 16     loss: 0.5988996624946594\n",
      "epoch: 17     loss: 0.5931522846221924\n",
      "epoch: 18     loss: 0.5825529098510742\n",
      "epoch: 19     loss: 0.575816810131073\n",
      "epoch: 20     loss: 0.5719273686408997\n",
      "epoch: 21     loss: 0.5671470165252686\n",
      "epoch: 22     loss: 0.5641254782676697\n",
      "epoch: 23     loss: 0.5622921586036682\n",
      "epoch: 24     loss: 0.564382016658783\n",
      "epoch: 25     loss: 0.5620140433311462\n",
      "epoch: 26     loss: 0.560416042804718\n",
      "epoch: 27     loss: 0.5594174265861511\n",
      "epoch: 28     loss: 0.5595069527626038\n",
      "epoch: 29     loss: 0.5610724687576294\n",
      "epoch: 30     loss: 0.5573740005493164\n",
      "epoch: 31     loss: 0.5588488578796387\n",
      "epoch: 32     loss: 0.5592905879020691\n",
      "epoch: 33     loss: 0.5596017837524414\n",
      "epoch: 34     loss: 0.5599040985107422\n",
      "epoch: 35     loss: 0.5602843165397644\n",
      "epoch: 36     loss: 0.5602184534072876\n",
      "epoch: 37     loss: 0.5605247020721436\n",
      "epoch: 38     loss: 0.5633568167686462\n",
      "epoch: 39     loss: 0.5610554218292236\n",
      "epoch: 40     loss: 0.56087726354599\n",
      "epoch: 41     loss: 0.5612126588821411\n",
      "epoch: 42     loss: 0.5609826445579529\n",
      "epoch: 43     loss: 0.5603240728378296\n",
      "epoch: 44     loss: 0.5605552196502686\n",
      "epoch: 45     loss: 0.5793564915657043\n",
      "epoch: 46     loss: 0.5607866644859314\n",
      "epoch: 47     loss: 0.5592066049575806\n",
      "epoch: 48     loss: 0.5594292283058167\n",
      "epoch: 49     loss: 0.5593262314796448\n",
      "epoch: 50     loss: 0.5637907385826111\n",
      "epoch: 51     loss: 0.556331217288971\n",
      "epoch: 52     loss: 0.555069088935852\n",
      "epoch: 53     loss: 0.556956946849823\n",
      "epoch: 54     loss: 0.556658148765564\n",
      "epoch: 55     loss: 0.5564112067222595\n",
      "epoch: 56     loss: 0.5564525723457336\n",
      "epoch: 57     loss: 0.5536020398139954\n",
      "epoch: 58     loss: 0.5559359192848206\n",
      "epoch: 59     loss: 0.5555679202079773\n",
      "epoch: 60     loss: 0.5551441311836243\n",
      "epoch: 61     loss: 0.554739773273468\n",
      "epoch: 62     loss: 0.553288459777832\n",
      "epoch: 63     loss: 0.5540205240249634\n",
      "epoch: 64     loss: 0.5532906651496887\n",
      "epoch: 65     loss: 0.5532530546188354\n",
      "epoch: 66     loss: 0.5528156161308289\n",
      "epoch: 67     loss: 0.5506791472434998\n",
      "epoch: 68     loss: 0.5526017546653748\n",
      "epoch: 69     loss: 0.5512299537658691\n",
      "epoch: 70     loss: 0.5498077869415283\n",
      "epoch: 71     loss: 0.5519610047340393\n",
      "epoch: 72     loss: 0.5495746731758118\n",
      "epoch: 73     loss: 0.5491273403167725\n",
      "epoch: 74     loss: 0.5496677160263062\n",
      "epoch: 75     loss: 0.5469613075256348\n",
      "epoch: 76     loss: 0.5475905537605286\n",
      "epoch: 77     loss: 0.5467847585678101\n",
      "epoch: 78     loss: 0.546498715877533\n",
      "epoch: 79     loss: 0.5436503291130066\n",
      "epoch: 80     loss: 0.542198121547699\n",
      "epoch: 81     loss: 0.5434869527816772\n",
      "epoch: 82     loss: 0.548998236656189\n",
      "epoch: 83     loss: 0.5411885380744934\n",
      "epoch: 84     loss: 0.5492676496505737\n",
      "epoch: 85     loss: 0.5410915613174438\n",
      "epoch: 86     loss: 0.5410098433494568\n",
      "epoch: 87     loss: 0.5392714738845825\n",
      "epoch: 88     loss: 0.5352073907852173\n",
      "epoch: 89     loss: 0.5328662991523743\n",
      "epoch: 90     loss: 0.5318376421928406\n",
      "epoch: 91     loss: 0.5295369029045105\n",
      "epoch: 92     loss: 0.5289342403411865\n",
      "epoch: 93     loss: 0.5268654227256775\n",
      "epoch: 94     loss: 0.526031494140625\n",
      "epoch: 95     loss: 0.525092601776123\n",
      "epoch: 96     loss: 0.5227226614952087\n",
      "epoch: 97     loss: 0.5220360159873962\n",
      "epoch: 98     loss: 0.5197319984436035\n",
      "epoch: 99     loss: 0.518934965133667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        x, y = HRdataset[i * batch: i * batch + batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用DataLoader重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch DataLoader负责管理批次，DataLoader从Dataset创建，自动为我们提供每个小批量，使遍历批次变得更容易，无需使用`HRdataset[i * batch: i * batch + batch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "HRdataloader = DataLoader(HRdataset, batch_size=batch) # batch_size: 每个批次的大小为，shuffle: 是否打乱数据\n",
    "\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.7839525938034058\n",
      "epoch: 1     loss: 0.8056097626686096\n",
      "epoch: 2     loss: 0.8044900894165039\n",
      "epoch: 3     loss: 0.786607563495636\n",
      "epoch: 4     loss: 0.7772766947746277\n",
      "epoch: 5     loss: 0.7672047019004822\n",
      "epoch: 6     loss: 0.7565413117408752\n",
      "epoch: 7     loss: 0.7444291114807129\n",
      "epoch: 8     loss: 0.7335003614425659\n",
      "epoch: 9     loss: 0.722275972366333\n",
      "epoch: 10     loss: 0.7108554840087891\n",
      "epoch: 11     loss: 0.6993186473846436\n",
      "epoch: 12     loss: 0.6877512335777283\n",
      "epoch: 13     loss: 0.6762359142303467\n",
      "epoch: 14     loss: 0.664892852306366\n",
      "epoch: 15     loss: 0.6529852151870728\n",
      "epoch: 16     loss: 0.6387475728988647\n",
      "epoch: 17     loss: 0.630553662776947\n",
      "epoch: 18     loss: 0.6199312806129456\n",
      "epoch: 19     loss: 0.6728217601776123\n",
      "epoch: 20     loss: 0.6396542191505432\n",
      "epoch: 21     loss: 0.621818482875824\n",
      "epoch: 22     loss: 0.5924992561340332\n",
      "epoch: 23     loss: 0.5756534934043884\n",
      "epoch: 24     loss: 0.5921425223350525\n",
      "epoch: 25     loss: 0.6098093390464783\n",
      "epoch: 26     loss: 0.5783880949020386\n",
      "epoch: 27     loss: 0.5730538964271545\n",
      "epoch: 28     loss: 0.5686180591583252\n",
      "epoch: 29     loss: 0.5646478533744812\n",
      "epoch: 30     loss: 0.5619305968284607\n",
      "epoch: 31     loss: 0.5596824288368225\n",
      "epoch: 32     loss: 0.5722330808639526\n",
      "epoch: 33     loss: 0.5611253380775452\n",
      "epoch: 34     loss: 0.5572251081466675\n",
      "epoch: 35     loss: 0.557214081287384\n",
      "epoch: 36     loss: 0.5573450326919556\n",
      "epoch: 37     loss: 0.5575399398803711\n",
      "epoch: 38     loss: 0.5576135516166687\n",
      "epoch: 39     loss: 0.5571624040603638\n",
      "epoch: 40     loss: 0.557407796382904\n",
      "epoch: 41     loss: 0.5586892366409302\n",
      "epoch: 42     loss: 0.5594194531440735\n",
      "epoch: 43     loss: 0.5598184466362\n",
      "epoch: 44     loss: 0.5600504875183105\n",
      "epoch: 45     loss: 0.5606282353401184\n",
      "epoch: 46     loss: 0.5609740614891052\n",
      "epoch: 47     loss: 0.5610805749893188\n",
      "epoch: 48     loss: 0.5601945519447327\n",
      "epoch: 49     loss: 0.5591132044792175\n",
      "epoch: 50     loss: 0.5598016977310181\n",
      "epoch: 51     loss: 0.559847891330719\n",
      "epoch: 52     loss: 0.5616791844367981\n",
      "epoch: 53     loss: 0.5591384768486023\n",
      "epoch: 54     loss: 0.5598530173301697\n",
      "epoch: 55     loss: 0.5601555705070496\n",
      "epoch: 56     loss: 0.5600084662437439\n",
      "epoch: 57     loss: 0.5587093234062195\n",
      "epoch: 58     loss: 0.5599209070205688\n",
      "epoch: 59     loss: 0.5593195557594299\n",
      "epoch: 60     loss: 0.5595328211784363\n",
      "epoch: 61     loss: 0.5665843486785889\n",
      "epoch: 62     loss: 0.5594968199729919\n",
      "epoch: 63     loss: 0.5588199496269226\n",
      "epoch: 64     loss: 0.5594707727432251\n",
      "epoch: 65     loss: 0.5586854815483093\n",
      "epoch: 66     loss: 0.5572300553321838\n",
      "epoch: 67     loss: 0.5612069964408875\n",
      "epoch: 68     loss: 0.570880651473999\n",
      "epoch: 69     loss: 0.5594897270202637\n",
      "epoch: 70     loss: 0.558427095413208\n",
      "epoch: 71     loss: 0.5579500198364258\n",
      "epoch: 72     loss: 0.5578715205192566\n",
      "epoch: 73     loss: 0.557334303855896\n",
      "epoch: 74     loss: 0.5563433170318604\n",
      "epoch: 75     loss: 0.5567870140075684\n",
      "epoch: 76     loss: 0.5556672811508179\n",
      "epoch: 77     loss: 0.5555264949798584\n",
      "epoch: 78     loss: 0.5562865734100342\n",
      "epoch: 79     loss: 0.5554083585739136\n",
      "epoch: 80     loss: 0.5546674728393555\n",
      "epoch: 81     loss: 0.5531834959983826\n",
      "epoch: 82     loss: 0.5537616610527039\n",
      "epoch: 83     loss: 0.5520901083946228\n",
      "epoch: 84     loss: 0.5532515048980713\n",
      "epoch: 85     loss: 0.5513628125190735\n",
      "epoch: 86     loss: 0.5523463487625122\n",
      "epoch: 87     loss: 0.5506049394607544\n",
      "epoch: 88     loss: 0.55194091796875\n",
      "epoch: 89     loss: 0.5503394603729248\n",
      "epoch: 90     loss: 0.5509408116340637\n",
      "epoch: 91     loss: 0.549521803855896\n",
      "epoch: 92     loss: 0.5519932508468628\n",
      "epoch: 93     loss: 0.5496670603752136\n",
      "epoch: 94     loss: 0.5587952136993408\n",
      "epoch: 95     loss: 0.5502967834472656\n",
      "epoch: 96     loss: 0.5499772429466248\n",
      "epoch: 97     loss: 0.5480579137802124\n",
      "epoch: 98     loss: 0.583207368850708\n",
      "epoch: 99     loss: 0.554233968257904\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x, y in HRdataloader:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分验证数据集和测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**过拟合：** 指模型在训练数据上表现良好，但在验证数据（未知数据）上表现不佳。  \n",
    "**欠拟合：** 指模型在训练数据上表现不佳，在验证数据上表现不佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们只是试图建立一个合理的训练循环以用于我们的训练数据。实际上，始终还应该具有一个验证集，以识别是否过度拟合。\n",
    "\n",
    "训练数据的乱序（shuffle）对于防止批次与过度拟合之间的相关性很重要。另一方面，无论我们是否乱序验证集，验证损失都是相同的。由于shufle需要额外的开销，因此shuffle验证数据没有任何意义。\n",
    "\n",
    "我们将为验证集使用批大小，该批大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。我们利用这一优势来使用更大的批量，并更快地计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11249, 20) (11249, 1) (3750, 20) (3750, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_data, Y_data) # 划分训练集和测试集\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将numpy数组转换为PyTorch张量\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_y = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     train_x = train_x.to('cuda')\n",
    "#     test_x = test_x.to('cuda')\n",
    "#     train_y = train_y.to('cuda')\n",
    "#     test_y = test_y.to('cuda')\n",
    "#     print('Using GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PyTorch的TensorDataset和DataLoader将数据集转换为数据加载器\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch)\n",
    "\n",
    "valid_ds = TensorDataset(test_x, test_y)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 准确率计算函数，用于计算预测值和真实值之间的准确率\n",
    "def accuracy(y_pred, y_true):\n",
    "    # 将预测值大于0.5的值转换为1，其余的转换为0\n",
    "    y_pred = (y_pred>0.5).type(torch.IntTensor)\n",
    "    # y_pred = (y_pred>0.5).type(torch.IntTensor).to('cuda')\n",
    "    # 计算预测值和真实值相等的数量，并转换为浮点数\n",
    "    return (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.564     accuracy: 0.761     test_loss: 0.562     test_accuracy: 0.764\n",
      "epoch: 1     loss: 0.56     accuracy: 0.761     test_loss: 0.559     test_accuracy: 0.764\n",
      "epoch: 2     loss: 0.556     accuracy: 0.761     test_loss: 0.555     test_accuracy: 0.764\n",
      "epoch: 3     loss: 0.552     accuracy: 0.761     test_loss: 0.55     test_accuracy: 0.764\n",
      "epoch: 4     loss: 0.547     accuracy: 0.761     test_loss: 0.545     test_accuracy: 0.764\n",
      "epoch: 5     loss: 0.541     accuracy: 0.761     test_loss: 0.54     test_accuracy: 0.764\n",
      "epoch: 6     loss: 0.535     accuracy: 0.761     test_loss: 0.535     test_accuracy: 0.764\n",
      "epoch: 7     loss: 0.528     accuracy: 0.761     test_loss: 0.527     test_accuracy: 0.764\n",
      "epoch: 8     loss: 0.52     accuracy: 0.761     test_loss: 0.52     test_accuracy: 0.764\n",
      "epoch: 9     loss: 0.512     accuracy: 0.761     test_loss: 0.512     test_accuracy: 0.764\n",
      "epoch: 10     loss: 0.504     accuracy: 0.761     test_loss: 0.504     test_accuracy: 0.764\n",
      "epoch: 11     loss: 0.495     accuracy: 0.761     test_loss: 0.495     test_accuracy: 0.764\n",
      "epoch: 12     loss: 0.486     accuracy: 0.761     test_loss: 0.487     test_accuracy: 0.764\n",
      "epoch: 13     loss: 0.476     accuracy: 0.761     test_loss: 0.477     test_accuracy: 0.764\n",
      "epoch: 14     loss: 0.466     accuracy: 0.76     test_loss: 0.468     test_accuracy: 0.762\n",
      "epoch: 15     loss: 0.456     accuracy: 0.759     test_loss: 0.458     test_accuracy: 0.76\n",
      "epoch: 16     loss: 0.447     accuracy: 0.757     test_loss: 0.449     test_accuracy: 0.759\n",
      "epoch: 17     loss: 0.438     accuracy: 0.764     test_loss: 0.44     test_accuracy: 0.763\n",
      "epoch: 18     loss: 0.428     accuracy: 0.769     test_loss: 0.43     test_accuracy: 0.768\n",
      "epoch: 19     loss: 0.42     accuracy: 0.783     test_loss: 0.422     test_accuracy: 0.781\n",
      "epoch: 20     loss: 0.41     accuracy: 0.789     test_loss: 0.412     test_accuracy: 0.787\n",
      "epoch: 21     loss: 0.402     accuracy: 0.815     test_loss: 0.404     test_accuracy: 0.807\n",
      "epoch: 22     loss: 0.394     accuracy: 0.825     test_loss: 0.396     test_accuracy: 0.818\n",
      "epoch: 23     loss: 0.387     accuracy: 0.836     test_loss: 0.389     test_accuracy: 0.833\n",
      "epoch: 24     loss: 0.381     accuracy: 0.848     test_loss: 0.383     test_accuracy: 0.845\n",
      "epoch: 25     loss: 0.374     accuracy: 0.853     test_loss: 0.377     test_accuracy: 0.849\n",
      "epoch: 26     loss: 0.368     accuracy: 0.855     test_loss: 0.37     test_accuracy: 0.852\n",
      "epoch: 27     loss: 0.362     accuracy: 0.857     test_loss: 0.365     test_accuracy: 0.854\n",
      "epoch: 28     loss: 0.356     accuracy: 0.861     test_loss: 0.359     test_accuracy: 0.858\n",
      "epoch: 29     loss: 0.351     accuracy: 0.863     test_loss: 0.354     test_accuracy: 0.859\n",
      "epoch: 30     loss: 0.345     accuracy: 0.865     test_loss: 0.348     test_accuracy: 0.86\n",
      "epoch: 31     loss: 0.341     accuracy: 0.869     test_loss: 0.344     test_accuracy: 0.865\n",
      "epoch: 32     loss: 0.334     accuracy: 0.871     test_loss: 0.337     test_accuracy: 0.867\n",
      "epoch: 33     loss: 0.33     accuracy: 0.874     test_loss: 0.333     test_accuracy: 0.871\n",
      "epoch: 34     loss: 0.327     accuracy: 0.878     test_loss: 0.331     test_accuracy: 0.877\n",
      "epoch: 35     loss: 0.323     accuracy: 0.88     test_loss: 0.326     test_accuracy: 0.878\n",
      "epoch: 36     loss: 0.32     accuracy: 0.882     test_loss: 0.324     test_accuracy: 0.879\n",
      "epoch: 37     loss: 0.316     accuracy: 0.883     test_loss: 0.32     test_accuracy: 0.88\n",
      "epoch: 38     loss: 0.312     accuracy: 0.884     test_loss: 0.316     test_accuracy: 0.882\n",
      "epoch: 39     loss: 0.309     accuracy: 0.886     test_loss: 0.314     test_accuracy: 0.881\n",
      "epoch: 40     loss: 0.306     accuracy: 0.887     test_loss: 0.311     test_accuracy: 0.884\n",
      "epoch: 41     loss: 0.304     accuracy: 0.888     test_loss: 0.309     test_accuracy: 0.884\n",
      "epoch: 42     loss: 0.301     accuracy: 0.888     test_loss: 0.306     test_accuracy: 0.884\n",
      "epoch: 43     loss: 0.299     accuracy: 0.89     test_loss: 0.304     test_accuracy: 0.886\n",
      "epoch: 44     loss: 0.297     accuracy: 0.891     test_loss: 0.302     test_accuracy: 0.888\n",
      "epoch: 45     loss: 0.295     accuracy: 0.892     test_loss: 0.3     test_accuracy: 0.89\n",
      "epoch: 46     loss: 0.292     accuracy: 0.893     test_loss: 0.298     test_accuracy: 0.889\n",
      "epoch: 47     loss: 0.29     accuracy: 0.894     test_loss: 0.296     test_accuracy: 0.891\n",
      "epoch: 48     loss: 0.288     accuracy: 0.895     test_loss: 0.294     test_accuracy: 0.892\n",
      "epoch: 49     loss: 0.287     accuracy: 0.894     test_loss: 0.293     test_accuracy: 0.891\n",
      "epoch: 50     loss: 0.285     accuracy: 0.895     test_loss: 0.291     test_accuracy: 0.892\n",
      "epoch: 51     loss: 0.284     accuracy: 0.895     test_loss: 0.29     test_accuracy: 0.891\n",
      "epoch: 52     loss: 0.282     accuracy: 0.895     test_loss: 0.289     test_accuracy: 0.892\n",
      "epoch: 53     loss: 0.281     accuracy: 0.895     test_loss: 0.288     test_accuracy: 0.891\n",
      "epoch: 54     loss: 0.279     accuracy: 0.895     test_loss: 0.286     test_accuracy: 0.893\n",
      "epoch: 55     loss: 0.278     accuracy: 0.896     test_loss: 0.285     test_accuracy: 0.893\n",
      "epoch: 56     loss: 0.277     accuracy: 0.896     test_loss: 0.284     test_accuracy: 0.892\n",
      "epoch: 57     loss: 0.275     accuracy: 0.897     test_loss: 0.282     test_accuracy: 0.894\n",
      "epoch: 58     loss: 0.275     accuracy: 0.896     test_loss: 0.282     test_accuracy: 0.893\n",
      "epoch: 59     loss: 0.273     accuracy: 0.897     test_loss: 0.28     test_accuracy: 0.895\n",
      "epoch: 60     loss: 0.273     accuracy: 0.896     test_loss: 0.28     test_accuracy: 0.894\n",
      "epoch: 61     loss: 0.271     accuracy: 0.897     test_loss: 0.279     test_accuracy: 0.896\n",
      "epoch: 62     loss: 0.271     accuracy: 0.897     test_loss: 0.279     test_accuracy: 0.895\n",
      "epoch: 63     loss: 0.27     accuracy: 0.897     test_loss: 0.278     test_accuracy: 0.896\n",
      "epoch: 64     loss: 0.269     accuracy: 0.898     test_loss: 0.277     test_accuracy: 0.897\n",
      "epoch: 65     loss: 0.268     accuracy: 0.898     test_loss: 0.276     test_accuracy: 0.898\n",
      "epoch: 66     loss: 0.266     accuracy: 0.9     test_loss: 0.274     test_accuracy: 0.899\n",
      "epoch: 67     loss: 0.266     accuracy: 0.9     test_loss: 0.274     test_accuracy: 0.899\n",
      "epoch: 68     loss: 0.265     accuracy: 0.898     test_loss: 0.273     test_accuracy: 0.898\n",
      "epoch: 69     loss: 0.264     accuracy: 0.899     test_loss: 0.272     test_accuracy: 0.899\n",
      "epoch: 70     loss: 0.263     accuracy: 0.899     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 71     loss: 0.263     accuracy: 0.9     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 72     loss: 0.263     accuracy: 0.899     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 73     loss: 0.262     accuracy: 0.9     test_loss: 0.27     test_accuracy: 0.899\n",
      "epoch: 74     loss: 0.261     accuracy: 0.901     test_loss: 0.269     test_accuracy: 0.9\n",
      "epoch: 75     loss: 0.261     accuracy: 0.9     test_loss: 0.269     test_accuracy: 0.899\n",
      "epoch: 76     loss: 0.26     accuracy: 0.901     test_loss: 0.268     test_accuracy: 0.901\n",
      "epoch: 77     loss: 0.259     accuracy: 0.901     test_loss: 0.267     test_accuracy: 0.901\n",
      "epoch: 78     loss: 0.259     accuracy: 0.9     test_loss: 0.267     test_accuracy: 0.901\n",
      "epoch: 79     loss: 0.258     accuracy: 0.902     test_loss: 0.266     test_accuracy: 0.902\n",
      "epoch: 80     loss: 0.257     accuracy: 0.902     test_loss: 0.265     test_accuracy: 0.902\n",
      "epoch: 81     loss: 0.256     accuracy: 0.902     test_loss: 0.265     test_accuracy: 0.902\n",
      "epoch: 82     loss: 0.256     accuracy: 0.902     test_loss: 0.264     test_accuracy: 0.901\n",
      "epoch: 83     loss: 0.257     accuracy: 0.9     test_loss: 0.265     test_accuracy: 0.901\n",
      "epoch: 84     loss: 0.255     accuracy: 0.903     test_loss: 0.264     test_accuracy: 0.902\n",
      "epoch: 85     loss: 0.254     accuracy: 0.903     test_loss: 0.263     test_accuracy: 0.903\n",
      "epoch: 86     loss: 0.254     accuracy: 0.902     test_loss: 0.263     test_accuracy: 0.902\n",
      "epoch: 87     loss: 0.254     accuracy: 0.901     test_loss: 0.263     test_accuracy: 0.902\n",
      "epoch: 88     loss: 0.253     accuracy: 0.902     test_loss: 0.262     test_accuracy: 0.903\n",
      "epoch: 89     loss: 0.253     accuracy: 0.902     test_loss: 0.262     test_accuracy: 0.903\n",
      "epoch: 90     loss: 0.253     accuracy: 0.901     test_loss: 0.262     test_accuracy: 0.902\n",
      "epoch: 91     loss: 0.252     accuracy: 0.902     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 92     loss: 0.252     accuracy: 0.901     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 93     loss: 0.252     accuracy: 0.902     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 94     loss: 0.25     accuracy: 0.903     test_loss: 0.26     test_accuracy: 0.904\n",
      "epoch: 95     loss: 0.249     accuracy: 0.903     test_loss: 0.259     test_accuracy: 0.905\n",
      "epoch: 96     loss: 0.249     accuracy: 0.903     test_loss: 0.259     test_accuracy: 0.906\n",
      "epoch: 97     loss: 0.249     accuracy: 0.903     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 98     loss: 0.249     accuracy: 0.903     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 99     loss: 0.248     accuracy: 0.904     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 100     loss: 0.248     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.907\n",
      "epoch: 101     loss: 0.248     accuracy: 0.902     test_loss: 0.258     test_accuracy: 0.905\n",
      "epoch: 102     loss: 0.247     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.906\n",
      "epoch: 103     loss: 0.247     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.906\n",
      "epoch: 104     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 105     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 106     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.906\n",
      "epoch: 107     loss: 0.245     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 108     loss: 0.245     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.906\n",
      "epoch: 109     loss: 0.245     accuracy: 0.905     test_loss: 0.254     test_accuracy: 0.906\n",
      "epoch: 110     loss: 0.245     accuracy: 0.905     test_loss: 0.254     test_accuracy: 0.907\n",
      "epoch: 111     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.907\n",
      "epoch: 112     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.906\n",
      "epoch: 113     loss: 0.245     accuracy: 0.904     test_loss: 0.254     test_accuracy: 0.906\n",
      "epoch: 114     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 115     loss: 0.244     accuracy: 0.904     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 116     loss: 0.243     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 117     loss: 0.243     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.906\n",
      "epoch: 118     loss: 0.243     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 119     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 120     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 121     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.905\n",
      "epoch: 122     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 123     loss: 0.242     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.905\n",
      "epoch: 124     loss: 0.241     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.907\n",
      "epoch: 125     loss: 0.241     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.906\n",
      "epoch: 126     loss: 0.241     accuracy: 0.904     test_loss: 0.251     test_accuracy: 0.906\n",
      "epoch: 127     loss: 0.241     accuracy: 0.905     test_loss: 0.25     test_accuracy: 0.906\n",
      "epoch: 128     loss: 0.24     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.906\n",
      "epoch: 129     loss: 0.24     accuracy: 0.905     test_loss: 0.25     test_accuracy: 0.906\n",
      "epoch: 130     loss: 0.24     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.906\n",
      "epoch: 131     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.907\n",
      "epoch: 132     loss: 0.239     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 133     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 134     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 135     loss: 0.24     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.907\n",
      "epoch: 136     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.909\n",
      "epoch: 137     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.908\n",
      "epoch: 138     loss: 0.237     accuracy: 0.907     test_loss: 0.247     test_accuracy: 0.91\n",
      "epoch: 139     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.908\n",
      "epoch: 140     loss: 0.238     accuracy: 0.906     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 141     loss: 0.237     accuracy: 0.906     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 142     loss: 0.237     accuracy: 0.907     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 143     loss: 0.237     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.908\n",
      "epoch: 144     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.909\n",
      "epoch: 145     loss: 0.236     accuracy: 0.908     test_loss: 0.246     test_accuracy: 0.909\n",
      "epoch: 146     loss: 0.237     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 147     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 148     loss: 0.236     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 149     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 150     loss: 0.236     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.908\n",
      "epoch: 151     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 152     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 153     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.909\n",
      "epoch: 154     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.909\n",
      "epoch: 155     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 156     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 157     loss: 0.234     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 158     loss: 0.235     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 159     loss: 0.234     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 160     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.907\n",
      "epoch: 161     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.907\n",
      "epoch: 162     loss: 0.234     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 163     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.908\n",
      "epoch: 164     loss: 0.234     accuracy: 0.909     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 165     loss: 0.234     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 166     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 167     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.909\n",
      "epoch: 168     loss: 0.233     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.907\n",
      "epoch: 169     loss: 0.234     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 170     loss: 0.233     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.908\n",
      "epoch: 171     loss: 0.232     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.909\n",
      "epoch: 172     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 173     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 174     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 175     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 176     loss: 0.232     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 177     loss: 0.232     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.909\n",
      "epoch: 178     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.91\n",
      "epoch: 179     loss: 0.231     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.91\n",
      "epoch: 180     loss: 0.232     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 181     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 182     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 183     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 184     loss: 0.231     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 185     loss: 0.23     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 186     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 187     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.91\n",
      "epoch: 188     loss: 0.231     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 189     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.91\n",
      "epoch: 190     loss: 0.231     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 191     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 192     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 193     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 194     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 195     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 196     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 197     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 198     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 199     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 200     loss: 0.23     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 201     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 202     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 203     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 204     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 205     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 206     loss: 0.229     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.91\n",
      "epoch: 207     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 208     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.909\n",
      "epoch: 209     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 210     loss: 0.229     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 211     loss: 0.228     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 212     loss: 0.229     accuracy: 0.911     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 213     loss: 0.229     accuracy: 0.911     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 214     loss: 0.228     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 215     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 216     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 217     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 218     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 219     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 220     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 221     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 222     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 223     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 224     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 225     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 226     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 227     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 228     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.911\n",
      "epoch: 229     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 230     loss: 0.226     accuracy: 0.912     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 231     loss: 0.226     accuracy: 0.912     test_loss: 0.235     test_accuracy: 0.911\n",
      "epoch: 232     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 233     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 234     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 235     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 236     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 237     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 238     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 239     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 240     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 241     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.911\n",
      "epoch: 242     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 243     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.911\n",
      "epoch: 244     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 245     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 246     loss: 0.225     accuracy: 0.914     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 247     loss: 0.225     accuracy: 0.913     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 248     loss: 0.225     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 249     loss: 0.225     accuracy: 0.913     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 250     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 251     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 252     loss: 0.225     accuracy: 0.914     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 253     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 254     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 255     loss: 0.225     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 256     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 257     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 258     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 259     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 260     loss: 0.224     accuracy: 0.915     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 261     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 262     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 263     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 264     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.909\n",
      "epoch: 265     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 266     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 267     loss: 0.223     accuracy: 0.915     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 268     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 269     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 270     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 271     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 272     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 273     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 274     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 275     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 276     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 277     loss: 0.223     accuracy: 0.915     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 278     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 279     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 280     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 281     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 282     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 283     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 284     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 285     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.913\n",
      "epoch: 286     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 287     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 288     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 289     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 290     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 291     loss: 0.222     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 292     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 293     loss: 0.221     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 294     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.912\n",
      "epoch: 295     loss: 0.22     accuracy: 0.918     test_loss: 0.229     test_accuracy: 0.914\n",
      "epoch: 296     loss: 0.22     accuracy: 0.918     test_loss: 0.229     test_accuracy: 0.914\n",
      "epoch: 297     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.912\n",
      "epoch: 298     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.914\n",
      "epoch: 299     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 300     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 301     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 302     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 303     loss: 0.22     accuracy: 0.917     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 304     loss: 0.22     accuracy: 0.917     test_loss: 0.228     test_accuracy: 0.914\n",
      "epoch: 305     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 306     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 307     loss: 0.219     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 308     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 309     loss: 0.219     accuracy: 0.919     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 310     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 311     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 312     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 313     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 314     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 315     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 316     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 317     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 318     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 319     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 320     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 321     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 322     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 323     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.914\n",
      "epoch: 324     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 325     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 326     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 327     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 328     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 329     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 330     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 331     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 332     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 333     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 334     loss: 0.217     accuracy: 0.919     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 335     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 336     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 337     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 338     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 339     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 340     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 341     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 342     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 343     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 344     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 345     loss: 0.216     accuracy: 0.92     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 346     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 347     loss: 0.216     accuracy: 0.921     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 348     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 349     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 350     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 351     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 352     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 353     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 354     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 355     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 356     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 357     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 358     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 359     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 360     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 361     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 362     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 363     loss: 0.214     accuracy: 0.922     test_loss: 0.222     test_accuracy: 0.918\n",
      "epoch: 364     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 365     loss: 0.214     accuracy: 0.922     test_loss: 0.222     test_accuracy: 0.918\n",
      "epoch: 366     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 367     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 368     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 369     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 370     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 371     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 372     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 373     loss: 0.213     accuracy: 0.921     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 374     loss: 0.214     accuracy: 0.922     test_loss: 0.221     test_accuracy: 0.918\n",
      "epoch: 375     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 376     loss: 0.213     accuracy: 0.922     test_loss: 0.221     test_accuracy: 0.919\n",
      "epoch: 377     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 378     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 379     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 380     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 381     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 382     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 383     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 384     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 385     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 386     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 387     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 388     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 389     loss: 0.211     accuracy: 0.923     test_loss: 0.219     test_accuracy: 0.92\n",
      "epoch: 390     loss: 0.211     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 391     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 392     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 393     loss: 0.211     accuracy: 0.923     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 394     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 395     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.92\n",
      "epoch: 396     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 397     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 398     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 399     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 400     loss: 0.211     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 401     loss: 0.21     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 402     loss: 0.21     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 403     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 404     loss: 0.21     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 405     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 406     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 407     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 408     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 409     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 410     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 411     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 412     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 413     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.92\n",
      "epoch: 414     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 415     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 416     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 417     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 418     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 419     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 420     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 421     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.921\n",
      "epoch: 422     loss: 0.209     accuracy: 0.924     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 423     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.921\n",
      "epoch: 424     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 425     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 426     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 427     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 428     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 429     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 430     loss: 0.208     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 431     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 432     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 433     loss: 0.208     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 434     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 435     loss: 0.207     accuracy: 0.923     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 436     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 437     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 438     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 439     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 440     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 441     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 442     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 443     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 444     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 445     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 446     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 447     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 448     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 449     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 450     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.924\n",
      "epoch: 451     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 452     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 453     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 454     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 455     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 456     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 457     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 458     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 459     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 460     loss: 0.205     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 461     loss: 0.205     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 462     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.923\n",
      "epoch: 463     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 464     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 465     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 466     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 467     loss: 0.204     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 468     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 469     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 470     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 471     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 472     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 473     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 474     loss: 0.203     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 475     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 476     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 477     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 478     loss: 0.203     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 479     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 480     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 481     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 482     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 483     loss: 0.203     accuracy: 0.926     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 484     loss: 0.203     accuracy: 0.926     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 485     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 486     loss: 0.203     accuracy: 0.925     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 487     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 488     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 489     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 490     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 491     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 492     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 493     loss: 0.202     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 494     loss: 0.202     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 495     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 496     loss: 0.201     accuracy: 0.927     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 497     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 498     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 499     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model, opt = get_model()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        epoch_accuracy = accuracy(model(train_x), train_y).data.item()\n",
    "        epoch_loss = loss_fn(model(train_x), train_y).data.item()\n",
    "\n",
    "        epoch_test_accuracy = accuracy(model(test_x), test_y).data.item()\n",
    "        epoch_test_loss = loss_fn(model(test_x), test_y).data.item()\n",
    "        print('epoch:', epoch, '   ', 'loss:', round(epoch_loss, 3),\n",
    "                               '   ', 'accuracy:', round(epoch_accuracy, 3),\n",
    "                               '   ', 'test_loss:', round(epoch_test_loss, 3),\n",
    "                               '   ', 'test_accuracy:', round(epoch_test_accuracy, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train()在训练之前调用代表训练模式\n",
    "\n",
    "model.eval() 推理之前进行调用代表推理模式\n",
    "\n",
    "不同的模式仅会在使用nn.BatchNorm2d ，nn.Dropout等层时以确保这些不同阶段的行为正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.5626) 0.76398027\n",
      "50 tensor(0.4614) 0.7658991\n",
      "100 tensor(0.4359) 0.7688597\n",
      "150 tensor(0.4314) 0.77615136\n",
      "200 tensor(0.4301) 0.7878701\n",
      "250 tensor(0.4296) 0.79047424\n",
      "300 tensor(0.4016) 0.81270564\n",
      "350 tensor(0.3325) 0.85316616\n",
      "400 tensor(0.2975) 0.880085\n",
      "450 tensor(0.2772) 0.8937226\n",
      "500 tensor(0.2623) 0.9035499\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin_1 = nn.Linear(20, 64)\n",
    "        self.lin_2 = nn.Linear(64, 64)\n",
    "        self.lin_3 = nn.Linear(64, 64)\n",
    "        self.lin_4 = nn.Linear(64, 1)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = self.lin_1(input)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_2(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_3(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.lin_4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.5699) 0.7612694 0.76398027\n",
      "50 tensor(0.3028) 0.8885359 0.88711625\n",
      "100 tensor(0.2569) 0.8999268 0.89590186\n",
      "150 tensor(0.2305) 0.9140698 0.907881\n",
      "200 tensor(0.2162) 0.922558 0.9176124\n",
      "250 tensor(0.2026) 0.9292164 0.9272478\n",
      "300 tensor(0.1902) 0.93398327 0.9314145\n",
      "350 tensor(0.1825) 0.936585 0.93584156\n",
      "400 tensor(0.1742) 0.9399586 0.93818533\n",
      "450 tensor(0.1671) 0.9411743 0.9426124\n",
      "500 tensor(0.1607) 0.94448626 0.9426124\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    if epoch%50==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_fn(model(xb), yb) for xb, yb in valid_dl)\n",
    "            acc_mean_train = np.mean([accuracy(model(xb), yb) for xb, yb in train_dl])\n",
    "            acc_mean_val = np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl])\n",
    "        acc_train.append(acc_mean_train)\n",
    "        acc_val.append(acc_mean_val)\n",
    "        print(epoch, valid_loss / len(valid_dl), acc_mean_train, acc_mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建fit（）和get_data（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在，我们获取数据加载器和拟合模型的整个过程可以在3行代码中运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5585223422050476\n",
      "1 0.5572936396916708\n",
      "2 0.5538846334775289\n",
      "3 0.5682887842814127\n",
      "4 0.5493621552149455\n",
      "5 0.5459549527486165\n",
      "6 0.5466750760714213\n",
      "7 0.5377705505688986\n",
      "8 0.5405092599232991\n",
      "9 0.5216653433799744\n",
      "10 0.514013319460551\n",
      "11 0.505571156279246\n",
      "12 0.49486463613510134\n",
      "13 0.48565271339416505\n",
      "14 0.4803563864072164\n",
      "15 0.45946247669855755\n",
      "16 0.4538744914849599\n",
      "17 0.43675982371966043\n",
      "18 0.42673156169255577\n",
      "19 0.4137803146839142\n",
      "20 0.40090010782877605\n",
      "21 0.3969560044765472\n",
      "22 0.39072483104070027\n",
      "23 0.37464483393033343\n",
      "24 0.3809296403090159\n",
      "25 0.35934452931086225\n",
      "26 0.35983266399701436\n",
      "27 0.3585698147296906\n",
      "28 0.3417404662768046\n",
      "29 0.34591260968844095\n",
      "30 0.33407240691185\n",
      "31 0.33202796533902484\n",
      "32 0.3291161770025889\n",
      "33 0.3334782658576965\n",
      "34 0.32432835887273154\n",
      "35 0.3379334044933319\n",
      "36 0.31954617563883464\n",
      "37 0.3172024084885915\n",
      "38 0.32336629778544107\n",
      "39 0.3180414479255676\n",
      "40 0.3144999252319336\n",
      "41 0.3151902012983958\n",
      "42 0.3093097085634867\n",
      "43 0.3082419154326121\n",
      "44 0.3223115423679352\n",
      "45 0.31921633052825926\n",
      "46 0.3068490433692932\n",
      "47 0.30507118695576985\n",
      "48 0.3066150216420492\n",
      "49 0.31398622533480325\n",
      "50 0.3005022413889567\n",
      "51 0.31158392429351806\n",
      "52 0.2981130225499471\n",
      "53 0.29831362727483113\n",
      "54 0.2951668711980184\n",
      "55 0.29535502429008487\n",
      "56 0.2945588399728139\n",
      "57 0.29201937905947367\n",
      "58 0.294459473323822\n",
      "59 0.34270676129659017\n",
      "60 0.2906801131884257\n",
      "61 0.29843041893641153\n",
      "62 0.28614356785615286\n",
      "63 0.2861232675552368\n",
      "64 0.2868414530913035\n",
      "65 0.2909357401847839\n",
      "66 0.2872243598461151\n",
      "67 0.28464978556632997\n",
      "68 0.2886512049039205\n",
      "69 0.2823302699327469\n",
      "70 0.30415485281944277\n",
      "71 0.2794848933458328\n",
      "72 0.27582421828111015\n",
      "73 0.27713309828440347\n",
      "74 0.27441644841035207\n",
      "75 0.27326079119841257\n",
      "76 0.2823368417739868\n",
      "77 0.27956587682565054\n",
      "78 0.28727005797227223\n",
      "79 0.2728370524009069\n",
      "80 0.2673796383857727\n",
      "81 0.26982505640188853\n",
      "82 0.26740491251150766\n",
      "83 0.2806004653374354\n",
      "84 0.26405525455474854\n",
      "85 0.2875447519222895\n",
      "86 0.2670623637119929\n",
      "87 0.2640748033761978\n",
      "88 0.2609626072963079\n",
      "89 0.2740634132623673\n",
      "90 0.25965993340015414\n",
      "91 0.25932604859670005\n",
      "92 0.2612945410569509\n",
      "93 0.2604571767171224\n",
      "94 0.2653060587565104\n",
      "95 0.253260853544871\n",
      "96 0.2664448384364446\n",
      "97 0.2502207060734431\n",
      "98 0.2621084857225418\n",
      "99 0.25229091106255846\n",
      "100 0.25300404080549876\n",
      "101 0.2508205281654994\n",
      "102 0.24763398209412893\n",
      "103 0.2473971748272578\n",
      "104 0.25068373436927793\n",
      "105 0.2454438776175181\n",
      "106 0.24494860401948293\n",
      "107 0.25518613006273905\n",
      "108 0.24288764804204305\n",
      "109 0.24681660269896188\n",
      "110 0.24633006014029185\n",
      "111 0.2499261644999186\n",
      "112 0.2378404653628667\n",
      "113 0.24560854743321736\n",
      "114 0.24114018348058064\n",
      "115 0.24241205321153006\n",
      "116 0.2359297441403071\n",
      "117 0.23532199802398682\n",
      "118 0.23400116215546926\n",
      "119 0.23662368795077007\n",
      "120 0.23362579923470814\n",
      "121 0.23495332430998483\n",
      "122 0.24244957163333894\n",
      "123 0.23091597005526224\n",
      "124 0.24327530415058135\n",
      "125 0.23127582342624664\n",
      "126 0.2318689723412196\n",
      "127 0.24960437478224437\n",
      "128 0.244589648381869\n",
      "129 0.23901406299273173\n",
      "130 0.23395179510116576\n",
      "131 0.2257823870976766\n",
      "132 0.22658760444323223\n",
      "133 0.23875550634860992\n",
      "134 0.22930609873930613\n",
      "135 0.23364042334556578\n",
      "136 0.2239091726620992\n",
      "137 0.23137693639596305\n",
      "138 0.22520298419793447\n",
      "139 0.22385783309936524\n",
      "140 0.2537149576028188\n",
      "141 0.22276502653757732\n",
      "142 0.2258105509519577\n",
      "143 0.22622659385204316\n",
      "144 0.23636961111227672\n",
      "145 0.2244742314974467\n",
      "146 0.22858230775992075\n",
      "147 0.2326254575252533\n",
      "148 0.22584648881753286\n",
      "149 0.22615100497404733\n",
      "150 0.24371969749927522\n",
      "151 0.22055255622069042\n",
      "152 0.28901809646288557\n",
      "153 0.2161103256861369\n",
      "154 0.21669896717071532\n",
      "155 0.21574656323591868\n",
      "156 0.2140825761715571\n",
      "157 0.2358282768646876\n",
      "158 0.22884450760682423\n",
      "159 0.21980532744725545\n",
      "160 0.23843070018291473\n",
      "161 0.2140421481291453\n",
      "162 0.2119106022675832\n",
      "163 0.21483991974989572\n",
      "164 0.22485180858771006\n",
      "165 0.2128124361673991\n",
      "166 0.21667274177074433\n",
      "167 0.21245954898198446\n",
      "168 0.21462962562243143\n",
      "169 0.21430082062880199\n",
      "170 0.2106738646030426\n",
      "171 0.20925897444089253\n",
      "172 0.21190397259394328\n",
      "173 0.22099653158982596\n",
      "174 0.2077662531455358\n",
      "175 0.21030568747520448\n",
      "176 0.20971950476964316\n",
      "177 0.22672876121203106\n",
      "178 0.2123180369615555\n",
      "179 0.21853121167023976\n",
      "180 0.2299245432774226\n",
      "181 0.2118396887143453\n",
      "182 0.20895238780975342\n",
      "183 0.20914330158233643\n",
      "184 0.20866728043556212\n",
      "185 0.20639028119246164\n",
      "186 0.20798527014255525\n",
      "187 0.20762932745615642\n",
      "188 0.2070103972673416\n",
      "189 0.23075418142477672\n",
      "190 0.2119745995044708\n",
      "191 0.21430060412883759\n",
      "192 0.21360746874809264\n",
      "193 0.21062375818888346\n",
      "194 0.20577433167298634\n",
      "195 0.20352604248523712\n",
      "196 0.20683842308521272\n",
      "197 0.22458419924577078\n",
      "198 0.2169521273771922\n",
      "199 0.21369927736918132\n",
      "200 0.2035347759962082\n",
      "201 0.20490284316539764\n",
      "202 0.2064095456202825\n",
      "203 0.2469554962317149\n",
      "204 0.2007039104382197\n",
      "205 0.2114231417973836\n",
      "206 0.20262700583140056\n",
      "207 0.20179893356959025\n",
      "208 0.20902258451779684\n",
      "209 0.2021151975552241\n",
      "210 0.20551492190361023\n",
      "211 0.20038895421822867\n",
      "212 0.19991902649402618\n",
      "213 0.20030823066234588\n",
      "214 0.20274520413080852\n",
      "215 0.2019366065979004\n",
      "216 0.21911720740000407\n",
      "217 0.19866432230472564\n",
      "218 0.2007078161239624\n",
      "219 0.2134657748778661\n",
      "220 0.19615123394330342\n",
      "221 0.19728170999685923\n",
      "222 0.19867875374158223\n",
      "223 0.19650131440162658\n",
      "224 0.1994504923582077\n",
      "225 0.20341756609280903\n",
      "226 0.2062248444080353\n",
      "227 0.19545216426849366\n",
      "228 0.2021013166030248\n",
      "229 0.19746661801338194\n",
      "230 0.19520025233427685\n",
      "231 0.2112498592933019\n",
      "232 0.21197804205417634\n",
      "233 0.20042440036932627\n",
      "234 0.23286954344113667\n",
      "235 0.21259826318422953\n",
      "236 0.19550404176712036\n",
      "237 0.20046932905515036\n",
      "238 0.19529178283214568\n",
      "239 0.19749424164295196\n",
      "240 0.19639495533307394\n",
      "241 0.20288442737261456\n",
      "242 0.2045671398639679\n",
      "243 0.1953196513811747\n",
      "244 0.20325603627363842\n",
      "245 0.2092506500005722\n",
      "246 0.20279141434033712\n",
      "247 0.22769737446308136\n",
      "248 0.19933339766661326\n",
      "249 0.1905278973499934\n",
      "250 0.19812692164580029\n",
      "251 0.19271294004917144\n",
      "252 0.20365473334789277\n",
      "253 0.1921919289032618\n",
      "254 0.19535814839204152\n",
      "255 0.19396585381031037\n",
      "256 0.2185350818077723\n",
      "257 0.19336743588447572\n",
      "258 0.1934505257924398\n",
      "259 0.19131531569163004\n",
      "260 0.19255936450958253\n",
      "261 0.2027739328543345\n",
      "262 0.1944264723221461\n",
      "263 0.19951157321135202\n",
      "264 0.19131920490264892\n",
      "265 0.19226147530873616\n",
      "266 0.1971974989016851\n",
      "267 0.18702876382668812\n",
      "268 0.19137440884113313\n",
      "269 0.19149618524710338\n",
      "270 0.1942850826183955\n",
      "271 0.20057806335290274\n",
      "272 0.18736620439688365\n",
      "273 0.21876310183207193\n",
      "274 0.1957652335802714\n",
      "275 0.18767399417559305\n",
      "276 0.19613946752548217\n",
      "277 0.19007018496990205\n",
      "278 0.1903711992343267\n",
      "279 0.18876120324929554\n",
      "280 0.1945504303296407\n",
      "281 0.1989893303155899\n",
      "282 0.18589371016820272\n",
      "283 0.18483463068803152\n",
      "284 0.24910510716438294\n",
      "285 0.1935402790228526\n",
      "286 0.19800676585833232\n",
      "287 0.18506188122431438\n",
      "288 0.1895740906318029\n",
      "289 0.18854400333563487\n",
      "290 0.18904208807150522\n",
      "291 0.1846797871351242\n",
      "292 0.1841294611374537\n",
      "293 0.18843856257597605\n",
      "294 0.18818542652130127\n",
      "295 0.18661362654368083\n",
      "296 0.1874954026301702\n",
      "297 0.18730594747066498\n",
      "298 0.1823210567077001\n",
      "299 0.18994318311214448\n",
      "300 0.18755146538416545\n",
      "301 0.18249364222685496\n",
      "302 0.2080941473722458\n",
      "303 0.21210515945752462\n",
      "304 0.18570846095879873\n",
      "305 0.18107493092219035\n",
      "306 0.19369440258344015\n",
      "307 0.1843664975007375\n",
      "308 0.1869347139676412\n",
      "309 0.1884587692817052\n",
      "310 0.18649291003545126\n",
      "311 0.17938957092761992\n",
      "312 0.18453159773349762\n",
      "313 0.18627145923773447\n",
      "314 0.187539551281929\n",
      "315 0.1810542840242386\n",
      "316 0.18591981928348542\n",
      "317 0.18927666312058766\n",
      "318 0.17835491902828216\n",
      "319 0.18292397849559783\n",
      "320 0.19822320148150127\n",
      "321 0.18808229865233103\n",
      "322 0.19014365273316702\n",
      "323 0.189238356312116\n",
      "324 0.18257074865500134\n",
      "325 0.18063321500619253\n",
      "326 0.17697757720152538\n",
      "327 0.18333935705025992\n",
      "328 0.1820164487918218\n",
      "329 0.1792444004456202\n",
      "330 0.18238454801241558\n",
      "331 0.18030265704790752\n",
      "332 0.18389701886177062\n",
      "333 0.17939964764912922\n",
      "334 0.18505801314512887\n",
      "335 0.1941799018462499\n",
      "336 0.1845243283033371\n",
      "337 0.23189160314400992\n",
      "338 0.18896385494470597\n",
      "339 0.17845241448879243\n",
      "340 0.18100439615249633\n",
      "341 0.18305899906158446\n",
      "342 0.1808627856095632\n",
      "343 0.18508298486073813\n",
      "344 0.19664022175470988\n",
      "345 0.17979134803613028\n",
      "346 0.17571488958199818\n",
      "347 0.18146691086292266\n",
      "348 0.1787819339354833\n",
      "349 0.18661702608267466\n",
      "350 0.17622592215538024\n",
      "351 0.18996235751310983\n",
      "352 0.18289446458816527\n",
      "353 0.17568172080516814\n",
      "354 0.21995702765782674\n",
      "355 0.18095674764315287\n",
      "356 0.19762183324495952\n",
      "357 0.17588766079743703\n",
      "358 0.1736857360204061\n",
      "359 0.17848864133358003\n",
      "360 0.17427322393258413\n",
      "361 0.184373672580719\n",
      "362 0.17792223206361135\n",
      "363 0.17955657064119976\n",
      "364 0.17589202392896017\n",
      "365 0.17965000567436218\n",
      "366 0.17706356461842854\n",
      "367 0.1798998727718989\n",
      "368 0.1764375643690427\n",
      "369 0.1899393554051717\n",
      "370 0.18356270341475805\n",
      "371 0.17734381892681123\n",
      "372 0.19154539771874746\n",
      "373 0.17489734439055124\n",
      "374 0.18526102151076\n",
      "375 0.17927602609793344\n",
      "376 0.17214716777006786\n",
      "377 0.17727710437774657\n",
      "378 0.18127041806379954\n",
      "379 0.17483943506479263\n",
      "380 0.1833483274936676\n",
      "381 0.17468488501707713\n",
      "382 0.18296310867468515\n",
      "383 0.17105145258903504\n",
      "384 0.17361242417494455\n",
      "385 0.17571343539555867\n",
      "386 0.17340954599380493\n",
      "387 0.17921572055021923\n",
      "388 0.1745311464945475\n",
      "389 0.17108316040039062\n",
      "390 0.17643794306119284\n",
      "391 0.18480299739837647\n",
      "392 0.17647357564767202\n",
      "393 0.17213850331306457\n",
      "394 0.17074834372997283\n",
      "395 0.17249165579477946\n",
      "396 0.17261232419013978\n",
      "397 0.1692234257777532\n",
      "398 0.16945370705922444\n",
      "399 0.17269981473286947\n",
      "400 0.1742480019013087\n",
      "401 0.18703141494989395\n",
      "402 0.1727712081193924\n",
      "403 0.1672994993607203\n",
      "404 0.18611815010309218\n",
      "405 0.18143438278834026\n",
      "406 0.17653207573890686\n",
      "407 0.17150497076511384\n",
      "408 0.17281218365033468\n",
      "409 0.17155854404767354\n",
      "410 0.1709978149652481\n",
      "411 0.18427735090255737\n",
      "412 0.16775608299573264\n",
      "413 0.16429924767812093\n",
      "414 0.16722007408936818\n",
      "415 0.168671983953317\n",
      "416 0.1662532138109207\n",
      "417 0.16959387729565303\n",
      "418 0.16960343168576558\n",
      "419 0.1690521247069041\n",
      "420 0.16391239952246348\n",
      "421 0.17380806602636972\n",
      "422 0.1640674975713094\n",
      "423 0.16738286906083424\n",
      "424 0.17708028308550516\n",
      "425 0.18760693390369415\n",
      "426 0.1660351802825928\n",
      "427 0.16844158386389416\n",
      "428 0.16577283318837482\n",
      "429 0.1686317458152771\n",
      "430 0.169485365096728\n",
      "431 0.18028926148414612\n",
      "432 0.16382449966271717\n",
      "433 0.17105431814193725\n",
      "434 0.17512167717615762\n",
      "435 0.17484077751636506\n",
      "436 0.16320490068594615\n",
      "437 0.17282672047615052\n",
      "438 0.17533460261027017\n",
      "439 0.175217067249616\n",
      "440 0.16210055679480234\n",
      "441 0.16467095351219177\n",
      "442 0.1647830454667409\n",
      "443 0.163850070754687\n",
      "444 0.16264719711939493\n",
      "445 0.19144581094582874\n",
      "446 0.18572642231782277\n",
      "447 0.18437553079922994\n",
      "448 0.1735172228892644\n",
      "449 0.1645232137997945\n",
      "450 0.16671401540438335\n",
      "451 0.18823159529368083\n",
      "452 0.1622177495876948\n",
      "453 0.16979494953950247\n",
      "454 0.1656956848224004\n",
      "455 0.16662948157787322\n",
      "456 0.17278846940994264\n",
      "457 0.16763884882132213\n",
      "458 0.17051385825475057\n",
      "459 0.17440447657108307\n",
      "460 0.17311202601591746\n",
      "461 0.16012395664056142\n",
      "462 0.175367174077034\n",
      "463 0.18086997144222258\n",
      "464 0.16745007683436075\n",
      "465 0.16466827761729558\n",
      "466 0.19336475516955057\n",
      "467 0.16388984668254852\n",
      "468 0.1645703471660614\n",
      "469 0.16923681805928548\n",
      "470 0.16493428791364034\n",
      "471 0.1598445300102234\n",
      "472 0.16523135928312938\n",
      "473 0.16688107334772745\n",
      "474 0.1685889376481374\n",
      "475 0.16619284116427105\n",
      "476 0.16226795160770416\n",
      "477 0.16261706318855285\n",
      "478 0.16483474571704865\n",
      "479 0.1649413599650065\n",
      "480 0.1682441472450892\n",
      "481 0.16974958153565725\n",
      "482 0.16359169002374013\n",
      "483 0.1674204554796219\n",
      "484 0.16670353705883026\n",
      "485 0.16335558207829792\n",
      "486 0.15783559488455454\n",
      "487 0.16472956024010976\n",
      "488 0.1644974544366201\n",
      "489 0.15753265578746795\n",
      "490 0.18899553221066792\n",
      "491 0.15982666255633035\n",
      "492 0.1609692197561264\n",
      "493 0.16693289417028429\n",
      "494 0.16648441519737245\n",
      "495 0.16154491241375604\n",
      "496 0.15721232877572378\n",
      "497 0.16502426493962605\n",
      "498 0.15984159887631735\n",
      "499 0.16372356317043305\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, batch)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
