{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二分类问题与多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>part</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years   part  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./dataset/HR.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   part                   14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "['sales' 'accounting' 'hr' 'technical' 'support' 'management' 'IT'\n",
      " 'product_mng' 'marketing' 'RandD']\n",
      "['low' 'medium' 'high']\n"
     ]
    }
   ],
   "source": [
    "# 数据基本信息\n",
    "print(data.info())\n",
    "print(data.part.unique())\n",
    "print(data.salary.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>...</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  IT  RandD  \\\n",
       "0                   3              0     1                      0   0      0   \n",
       "1                   6              0     1                      0   0      0   \n",
       "2                   4              0     1                      0   0      0   \n",
       "3                   5              0     1                      0   0      0   \n",
       "4                   3              0     1                      0   0      0   \n",
       "\n",
       "   ...  hr  management  marketing  product_mng  sales  support  technical  \\\n",
       "0  ...   0           0          0            0      1        0          0   \n",
       "1  ...   0           0          0            0      1        0          0   \n",
       "2  ...   0           0          0            0      1        0          0   \n",
       "3  ...   0           0          0            0      1        0          0   \n",
       "4  ...   0           0          0            0      1        0          0   \n",
       "\n",
       "   high  low  medium  \n",
       "0     0    1       0  \n",
       "1     0    0       1  \n",
       "2     0    0       1  \n",
       "3     0    1       0  \n",
       "4     0    1       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 简单的数据分析\n",
    "# data.groupby(['salary', 'part']).size()\n",
    "# .get_dummies()方法可以将分类数据转换为one-hot编码\n",
    "data = data.join(pd.get_dummies(data.part).astype(int)).join(pd.get_dummies(data.salary).astype(int))\n",
    "# 删除原来的分类数据\n",
    "data.drop(columns=['part', 'salary'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   IT                     14999 non-null  int32  \n",
      " 9   RandD                  14999 non-null  int32  \n",
      " 10  accounting             14999 non-null  int32  \n",
      " 11  hr                     14999 non-null  int32  \n",
      " 12  management             14999 non-null  int32  \n",
      " 13  marketing              14999 non-null  int32  \n",
      " 14  product_mng            14999 non-null  int32  \n",
      " 15  sales                  14999 non-null  int32  \n",
      " 16  support                14999 non-null  int32  \n",
      " 17  technical              14999 non-null  int32  \n",
      " 18  high                   14999 non-null  int32  \n",
      " 19  low                    14999 non-null  int32  \n",
      " 20  medium                 14999 non-null  int32  \n",
      "dtypes: float64(2), int32(13), int64(6)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "0    11428\n",
      "1     3571\n",
      "Name: count, dtype: int64\n",
      "0.7619174611640777\n"
     ]
    }
   ],
   "source": [
    "# 查看离职率\n",
    "print(data.left.value_counts())\n",
    "# 全部预测为不离职\n",
    "print(data.left.value_counts()[0] / data.left.value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 1])\n"
     ]
    }
   ],
   "source": [
    "# 处理结果数据\n",
    "Y_data = data.left.values.reshape(-1, 1)\n",
    "Y = torch.from_numpy(Y_data).type(torch.FloatTensor)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14999, 20])\n"
     ]
    }
   ],
   "source": [
    "# 处理特征数据\n",
    "# 使用列表推导式，获取除了'left'列之外的所有列\n",
    "# [c for c in data.columns if c != 'left']\n",
    "# 使用.values方法，将DataFrame转换为numpy数组\n",
    "X_data = data[[c for c in data.columns if c != 'left']].values\n",
    "X = torch.from_numpy(X_data).type(torch.FloatTensor)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# # 自定义模型：逻辑回归模型\n",
    "# class Logistic(nn.Module):  # 继承nn.Module\n",
    "#     def __init__(self):     # 初始化所有的层\n",
    "#         super().__init__()  # 继承父类中所有的属性和方法\n",
    "#         self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "#         self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "#         self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "#         self.activate = nn.ReLU()       # 定义ReLU激活函数\n",
    "#         self.sigmoid = nn.Sigmoid()     # 定义Sigmoid激活函数\n",
    "#     def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "#         x = self.lin_1(input)   # 将输入数据传入第一层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_2(x)       # 将激活后的数据传入第二层线性层\n",
    "#         x = self.activate(x)    # ReLU激活函数\n",
    "#         x = self.lin_3(x)       # 将激活后的数据传入第三层线性层\n",
    "#         x = self.sigmoid(x)     # Sigmoid激活函数\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型改写\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # 函数式API，调用方便使代码更简洁\n",
    "class Logistic(nn.Module):  # 继承nn.Module\n",
    "    def __init__(self):     # 初始化所有的层\n",
    "        super().__init__()  # 继承父类中所有的属性和方法\n",
    "        self.lin_1 = nn.Linear(20, 64)  # 定义第一层线性层，输入维度为20，输出维度为64\n",
    "        self.lin_2 = nn.Linear(64, 64)  # 定义第二层线性层，输入维度为64，输出维度为64\n",
    "        self.lin_3 = nn.Linear(64, 1)   # 定义第三层线性层，输入维度为64，输出维度为1\n",
    "    def forward(self, input):   # 前向传播函数，定义模型的运算过程，覆盖父类中的forward方法\n",
    "        x = F.relu(self.lin_1(input))   # 将输入数据传入第一层线性层，并使用ReLU激活函数\n",
    "        x = F.relu(self.lin_2(x))       # 将激活后的数据传入第二层线性层，并使用ReLU激活函数\n",
    "        x = F.sigmoid(self.lin_3(x))     # 将激活后的数据传入第三层线性层，并使用Sigmoid激活函数\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Logistic(\n",
      "  (lin_1): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (lin_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_3): Linear(in_features=64, out_features=1, bias=True)\n",
      "), Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# 封装模型和优化器的创建，提高代码复用性\n",
    "lr = 0.0001\n",
    "def get_model():\n",
    "    model = Logistic()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, opt\n",
    "print(get_model())\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据集，分批次进行训练\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 手动分批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6653244495391846\n",
      "epoch: 1     loss: 0.6724502444267273\n",
      "epoch: 2     loss: 0.66326904296875\n",
      "epoch: 3     loss: 0.6563212871551514\n",
      "epoch: 4     loss: 0.6486781239509583\n",
      "epoch: 5     loss: 0.6963856816291809\n",
      "epoch: 6     loss: 0.6340402364730835\n",
      "epoch: 7     loss: 0.6262431144714355\n",
      "epoch: 8     loss: 0.6186566352844238\n",
      "epoch: 9     loss: 0.610680878162384\n",
      "epoch: 10     loss: 0.624306857585907\n",
      "epoch: 11     loss: 0.5956229567527771\n",
      "epoch: 12     loss: 0.5905640721321106\n",
      "epoch: 13     loss: 0.5859825611114502\n",
      "epoch: 14     loss: 0.5818182826042175\n",
      "epoch: 15     loss: 0.578037440776825\n",
      "epoch: 16     loss: 0.5848634839057922\n",
      "epoch: 17     loss: 0.5720465779304504\n",
      "epoch: 18     loss: 0.5694150328636169\n",
      "epoch: 19     loss: 0.5671862363815308\n",
      "epoch: 20     loss: 0.5652644038200378\n",
      "epoch: 21     loss: 0.5636811852455139\n",
      "epoch: 22     loss: 0.5623966455459595\n",
      "epoch: 23     loss: 0.5613612532615662\n",
      "epoch: 24     loss: 0.580359935760498\n",
      "epoch: 25     loss: 0.5601328015327454\n",
      "epoch: 26     loss: 0.5598888397216797\n",
      "epoch: 27     loss: 0.5596734881401062\n",
      "epoch: 28     loss: 0.5596132278442383\n",
      "epoch: 29     loss: 0.5596141219139099\n",
      "epoch: 30     loss: 0.559776782989502\n",
      "epoch: 31     loss: 0.5616737008094788\n",
      "epoch: 32     loss: 0.5591868758201599\n",
      "epoch: 33     loss: 0.5598408579826355\n",
      "epoch: 34     loss: 0.5615217685699463\n",
      "epoch: 35     loss: 0.5597636699676514\n",
      "epoch: 36     loss: 0.5620923042297363\n",
      "epoch: 37     loss: 0.5620797872543335\n",
      "epoch: 38     loss: 0.5608329772949219\n",
      "epoch: 39     loss: 0.5618199706077576\n",
      "epoch: 40     loss: 0.5620315074920654\n",
      "epoch: 41     loss: 0.5693978667259216\n",
      "epoch: 42     loss: 0.5613442063331604\n",
      "epoch: 43     loss: 0.5621647834777832\n",
      "epoch: 44     loss: 0.5621275901794434\n",
      "epoch: 45     loss: 0.5622699856758118\n",
      "epoch: 46     loss: 0.5628376603126526\n",
      "epoch: 47     loss: 0.5615686178207397\n",
      "epoch: 48     loss: 0.5624603033065796\n",
      "epoch: 49     loss: 0.5613117218017578\n",
      "epoch: 50     loss: 0.5620439052581787\n",
      "epoch: 51     loss: 0.5644301772117615\n",
      "epoch: 52     loss: 0.558138370513916\n",
      "epoch: 53     loss: 0.5606213808059692\n",
      "epoch: 54     loss: 0.5606685876846313\n",
      "epoch: 55     loss: 0.56052166223526\n",
      "epoch: 56     loss: 0.5652745962142944\n",
      "epoch: 57     loss: 0.5590802431106567\n",
      "epoch: 58     loss: 0.560001790523529\n",
      "epoch: 59     loss: 0.5599151849746704\n",
      "epoch: 60     loss: 0.5596983432769775\n",
      "epoch: 61     loss: 0.5594485998153687\n",
      "epoch: 62     loss: 0.5591833591461182\n",
      "epoch: 63     loss: 0.5589162707328796\n",
      "epoch: 64     loss: 0.558548092842102\n",
      "epoch: 65     loss: 0.5560385584831238\n",
      "epoch: 66     loss: 0.5577181577682495\n",
      "epoch: 67     loss: 0.5578632354736328\n",
      "epoch: 68     loss: 0.5576692223548889\n",
      "epoch: 69     loss: 0.5602340698242188\n",
      "epoch: 70     loss: 0.5588377714157104\n",
      "epoch: 71     loss: 0.5571046471595764\n",
      "epoch: 72     loss: 0.5568392276763916\n",
      "epoch: 73     loss: 0.556615948677063\n",
      "epoch: 74     loss: 0.5563758015632629\n",
      "epoch: 75     loss: 0.5561412572860718\n",
      "epoch: 76     loss: 0.5559143424034119\n",
      "epoch: 77     loss: 0.5556930899620056\n",
      "epoch: 78     loss: 0.5552341341972351\n",
      "epoch: 79     loss: 0.5728468298912048\n",
      "epoch: 80     loss: 0.5561484694480896\n",
      "epoch: 81     loss: 0.554599404335022\n",
      "epoch: 82     loss: 0.5539093017578125\n",
      "epoch: 83     loss: 0.5534027218818665\n",
      "epoch: 84     loss: 0.5530707240104675\n",
      "epoch: 85     loss: 0.5526936650276184\n",
      "epoch: 86     loss: 0.5523322224617004\n",
      "epoch: 87     loss: 0.5520640015602112\n",
      "epoch: 88     loss: 0.5540891885757446\n",
      "epoch: 89     loss: 0.5527768731117249\n",
      "epoch: 90     loss: 0.5522754788398743\n",
      "epoch: 91     loss: 0.5517933368682861\n",
      "epoch: 92     loss: 0.5529729127883911\n",
      "epoch: 93     loss: 0.5509827136993408\n",
      "epoch: 94     loss: 0.5506672859191895\n",
      "epoch: 95     loss: 0.5504059195518494\n",
      "epoch: 96     loss: 0.5502349734306335\n",
      "epoch: 97     loss: 0.5498477220535278\n",
      "epoch: 98     loss: 0.5494824647903442\n",
      "epoch: 99     loss: 0.5490240454673767\n"
     ]
    }
   ],
   "source": [
    "# 分批次循环训练\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):     # 按照批次进行训练\n",
    "        start = i * batch              # 每个批次的起始索引\n",
    "        end = start + batch            # 每个批次的结束索引\n",
    "        x = X[start: end]\n",
    "        y = Y[start: end]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute loss: BCELoss expects the target to be between 0 and 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # Gradient reset\n",
    "        opt.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用dataset重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch有一个抽象的Dataset类。Dataset可以是任何具有__len__函数和__getitem__作为对其进行索引的方法的函数。PyTorch的TensorDataset是一个包装张量的Dataset。通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。这将使我们在训练的同一行中更容易访问自变量和因变量。下面将自定义HRDataset类创建为的Dataset的子类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "# print(HRdataset[2: 5])\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.6955068111419678\n",
      "epoch: 1     loss: 0.6839310526847839\n",
      "epoch: 2     loss: 0.6755874752998352\n",
      "epoch: 3     loss: 0.6657107472419739\n",
      "epoch: 4     loss: 0.6551401615142822\n",
      "epoch: 5     loss: 0.6444115042686462\n",
      "epoch: 6     loss: 0.6337931752204895\n",
      "epoch: 7     loss: 0.6235368847846985\n",
      "epoch: 8     loss: 0.6138008236885071\n",
      "epoch: 9     loss: 0.618353009223938\n",
      "epoch: 10     loss: 0.6162864565849304\n",
      "epoch: 11     loss: 0.6000204682350159\n",
      "epoch: 12     loss: 0.5915645956993103\n",
      "epoch: 13     loss: 0.5849820375442505\n",
      "epoch: 14     loss: 0.5794337391853333\n",
      "epoch: 15     loss: 0.5755189657211304\n",
      "epoch: 16     loss: 0.5988996624946594\n",
      "epoch: 17     loss: 0.5931522846221924\n",
      "epoch: 18     loss: 0.5825529098510742\n",
      "epoch: 19     loss: 0.575816810131073\n",
      "epoch: 20     loss: 0.5719273686408997\n",
      "epoch: 21     loss: 0.5671470165252686\n",
      "epoch: 22     loss: 0.5641254782676697\n",
      "epoch: 23     loss: 0.5622921586036682\n",
      "epoch: 24     loss: 0.564382016658783\n",
      "epoch: 25     loss: 0.5620140433311462\n",
      "epoch: 26     loss: 0.560416042804718\n",
      "epoch: 27     loss: 0.5594174265861511\n",
      "epoch: 28     loss: 0.5595069527626038\n",
      "epoch: 29     loss: 0.5610724687576294\n",
      "epoch: 30     loss: 0.5573740005493164\n",
      "epoch: 31     loss: 0.5588488578796387\n",
      "epoch: 32     loss: 0.5592905879020691\n",
      "epoch: 33     loss: 0.5596017837524414\n",
      "epoch: 34     loss: 0.5599040985107422\n",
      "epoch: 35     loss: 0.5602843165397644\n",
      "epoch: 36     loss: 0.5602184534072876\n",
      "epoch: 37     loss: 0.5605247020721436\n",
      "epoch: 38     loss: 0.5633568167686462\n",
      "epoch: 39     loss: 0.5610554218292236\n",
      "epoch: 40     loss: 0.56087726354599\n",
      "epoch: 41     loss: 0.5612126588821411\n",
      "epoch: 42     loss: 0.5609826445579529\n",
      "epoch: 43     loss: 0.5603240728378296\n",
      "epoch: 44     loss: 0.5605552196502686\n",
      "epoch: 45     loss: 0.5793564915657043\n",
      "epoch: 46     loss: 0.5607866644859314\n",
      "epoch: 47     loss: 0.5592066049575806\n",
      "epoch: 48     loss: 0.5594292283058167\n",
      "epoch: 49     loss: 0.5593262314796448\n",
      "epoch: 50     loss: 0.5637907385826111\n",
      "epoch: 51     loss: 0.556331217288971\n",
      "epoch: 52     loss: 0.555069088935852\n",
      "epoch: 53     loss: 0.556956946849823\n",
      "epoch: 54     loss: 0.556658148765564\n",
      "epoch: 55     loss: 0.5564112067222595\n",
      "epoch: 56     loss: 0.5564525723457336\n",
      "epoch: 57     loss: 0.5536020398139954\n",
      "epoch: 58     loss: 0.5559359192848206\n",
      "epoch: 59     loss: 0.5555679202079773\n",
      "epoch: 60     loss: 0.5551441311836243\n",
      "epoch: 61     loss: 0.554739773273468\n",
      "epoch: 62     loss: 0.553288459777832\n",
      "epoch: 63     loss: 0.5540205240249634\n",
      "epoch: 64     loss: 0.5532906651496887\n",
      "epoch: 65     loss: 0.5532530546188354\n",
      "epoch: 66     loss: 0.5528156161308289\n",
      "epoch: 67     loss: 0.5506791472434998\n",
      "epoch: 68     loss: 0.5526017546653748\n",
      "epoch: 69     loss: 0.5512299537658691\n",
      "epoch: 70     loss: 0.5498077869415283\n",
      "epoch: 71     loss: 0.5519610047340393\n",
      "epoch: 72     loss: 0.5495746731758118\n",
      "epoch: 73     loss: 0.5491273403167725\n",
      "epoch: 74     loss: 0.5496677160263062\n",
      "epoch: 75     loss: 0.5469613075256348\n",
      "epoch: 76     loss: 0.5475905537605286\n",
      "epoch: 77     loss: 0.5467847585678101\n",
      "epoch: 78     loss: 0.546498715877533\n",
      "epoch: 79     loss: 0.5436503291130066\n",
      "epoch: 80     loss: 0.542198121547699\n",
      "epoch: 81     loss: 0.5434869527816772\n",
      "epoch: 82     loss: 0.548998236656189\n",
      "epoch: 83     loss: 0.5411885380744934\n",
      "epoch: 84     loss: 0.5492676496505737\n",
      "epoch: 85     loss: 0.5410915613174438\n",
      "epoch: 86     loss: 0.5410098433494568\n",
      "epoch: 87     loss: 0.5392714738845825\n",
      "epoch: 88     loss: 0.5352073907852173\n",
      "epoch: 89     loss: 0.5328662991523743\n",
      "epoch: 90     loss: 0.5318376421928406\n",
      "epoch: 91     loss: 0.5295369029045105\n",
      "epoch: 92     loss: 0.5289342403411865\n",
      "epoch: 93     loss: 0.5268654227256775\n",
      "epoch: 94     loss: 0.526031494140625\n",
      "epoch: 95     loss: 0.525092601776123\n",
      "epoch: 96     loss: 0.5227226614952087\n",
      "epoch: 97     loss: 0.5220360159873962\n",
      "epoch: 98     loss: 0.5197319984436035\n",
      "epoch: 99     loss: 0.518934965133667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        x, y = HRdataset[i * batch: i * batch + batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用DataLoader重构模型训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch DataLoader负责管理批次，DataLoader从Dataset创建，自动为我们提供每个小批量，使遍历批次变得更容易，无需使用`HRdataset[i * batch: i * batch + batch]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "HRdataloader = DataLoader(HRdataset, batch_size=batch) # batch_size: 每个批次的大小为，shuffle: 是否打乱数据\n",
    "\n",
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.7839525938034058\n",
      "epoch: 1     loss: 0.8056097626686096\n",
      "epoch: 2     loss: 0.8044900894165039\n",
      "epoch: 3     loss: 0.786607563495636\n",
      "epoch: 4     loss: 0.7772766947746277\n",
      "epoch: 5     loss: 0.7672047019004822\n",
      "epoch: 6     loss: 0.7565413117408752\n",
      "epoch: 7     loss: 0.7444291114807129\n",
      "epoch: 8     loss: 0.7335003614425659\n",
      "epoch: 9     loss: 0.722275972366333\n",
      "epoch: 10     loss: 0.7108554840087891\n",
      "epoch: 11     loss: 0.6993186473846436\n",
      "epoch: 12     loss: 0.6877512335777283\n",
      "epoch: 13     loss: 0.6762359142303467\n",
      "epoch: 14     loss: 0.664892852306366\n",
      "epoch: 15     loss: 0.6529852151870728\n",
      "epoch: 16     loss: 0.6387475728988647\n",
      "epoch: 17     loss: 0.630553662776947\n",
      "epoch: 18     loss: 0.6199312806129456\n",
      "epoch: 19     loss: 0.6728217601776123\n",
      "epoch: 20     loss: 0.6396542191505432\n",
      "epoch: 21     loss: 0.621818482875824\n",
      "epoch: 22     loss: 0.5924992561340332\n",
      "epoch: 23     loss: 0.5756534934043884\n",
      "epoch: 24     loss: 0.5921425223350525\n",
      "epoch: 25     loss: 0.6098093390464783\n",
      "epoch: 26     loss: 0.5783880949020386\n",
      "epoch: 27     loss: 0.5730538964271545\n",
      "epoch: 28     loss: 0.5686180591583252\n",
      "epoch: 29     loss: 0.5646478533744812\n",
      "epoch: 30     loss: 0.5619305968284607\n",
      "epoch: 31     loss: 0.5596824288368225\n",
      "epoch: 32     loss: 0.5722330808639526\n",
      "epoch: 33     loss: 0.5611253380775452\n",
      "epoch: 34     loss: 0.5572251081466675\n",
      "epoch: 35     loss: 0.557214081287384\n",
      "epoch: 36     loss: 0.5573450326919556\n",
      "epoch: 37     loss: 0.5575399398803711\n",
      "epoch: 38     loss: 0.5576135516166687\n",
      "epoch: 39     loss: 0.5571624040603638\n",
      "epoch: 40     loss: 0.557407796382904\n",
      "epoch: 41     loss: 0.5586892366409302\n",
      "epoch: 42     loss: 0.5594194531440735\n",
      "epoch: 43     loss: 0.5598184466362\n",
      "epoch: 44     loss: 0.5600504875183105\n",
      "epoch: 45     loss: 0.5606282353401184\n",
      "epoch: 46     loss: 0.5609740614891052\n",
      "epoch: 47     loss: 0.5610805749893188\n",
      "epoch: 48     loss: 0.5601945519447327\n",
      "epoch: 49     loss: 0.5591132044792175\n",
      "epoch: 50     loss: 0.5598016977310181\n",
      "epoch: 51     loss: 0.559847891330719\n",
      "epoch: 52     loss: 0.5616791844367981\n",
      "epoch: 53     loss: 0.5591384768486023\n",
      "epoch: 54     loss: 0.5598530173301697\n",
      "epoch: 55     loss: 0.5601555705070496\n",
      "epoch: 56     loss: 0.5600084662437439\n",
      "epoch: 57     loss: 0.5587093234062195\n",
      "epoch: 58     loss: 0.5599209070205688\n",
      "epoch: 59     loss: 0.5593195557594299\n",
      "epoch: 60     loss: 0.5595328211784363\n",
      "epoch: 61     loss: 0.5665843486785889\n",
      "epoch: 62     loss: 0.5594968199729919\n",
      "epoch: 63     loss: 0.5588199496269226\n",
      "epoch: 64     loss: 0.5594707727432251\n",
      "epoch: 65     loss: 0.5586854815483093\n",
      "epoch: 66     loss: 0.5572300553321838\n",
      "epoch: 67     loss: 0.5612069964408875\n",
      "epoch: 68     loss: 0.570880651473999\n",
      "epoch: 69     loss: 0.5594897270202637\n",
      "epoch: 70     loss: 0.558427095413208\n",
      "epoch: 71     loss: 0.5579500198364258\n",
      "epoch: 72     loss: 0.5578715205192566\n",
      "epoch: 73     loss: 0.557334303855896\n",
      "epoch: 74     loss: 0.5563433170318604\n",
      "epoch: 75     loss: 0.5567870140075684\n",
      "epoch: 76     loss: 0.5556672811508179\n",
      "epoch: 77     loss: 0.5555264949798584\n",
      "epoch: 78     loss: 0.5562865734100342\n",
      "epoch: 79     loss: 0.5554083585739136\n",
      "epoch: 80     loss: 0.5546674728393555\n",
      "epoch: 81     loss: 0.5531834959983826\n",
      "epoch: 82     loss: 0.5537616610527039\n",
      "epoch: 83     loss: 0.5520901083946228\n",
      "epoch: 84     loss: 0.5532515048980713\n",
      "epoch: 85     loss: 0.5513628125190735\n",
      "epoch: 86     loss: 0.5523463487625122\n",
      "epoch: 87     loss: 0.5506049394607544\n",
      "epoch: 88     loss: 0.55194091796875\n",
      "epoch: 89     loss: 0.5503394603729248\n",
      "epoch: 90     loss: 0.5509408116340637\n",
      "epoch: 91     loss: 0.549521803855896\n",
      "epoch: 92     loss: 0.5519932508468628\n",
      "epoch: 93     loss: 0.5496670603752136\n",
      "epoch: 94     loss: 0.5587952136993408\n",
      "epoch: 95     loss: 0.5502967834472656\n",
      "epoch: 96     loss: 0.5499772429466248\n",
      "epoch: 97     loss: 0.5480579137802124\n",
      "epoch: 98     loss: 0.583207368850708\n",
      "epoch: 99     loss: 0.554233968257904\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x, y in HRdataloader:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, '   ', 'loss:', loss_fn(model(X), Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分验证数据集和测试数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**过拟合：** 指模型在训练数据上表现良好，但在验证数据（未知数据）上表现不佳。  \n",
    "**欠拟合：** 指模型在训练数据上表现不佳，在验证数据上表现不佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们只是试图建立一个合理的训练循环以用于我们的训练数据。实际上，始终还应该具有一个验证集，以识别是否过度拟合。\n",
    "\n",
    "训练数据的乱序（shuffle）对于防止批次与过度拟合之间的相关性很重要。另一方面，无论我们是否乱序验证集，验证损失都是相同的。由于shufle需要额外的开销，因此shuffle验证数据没有任何意义。\n",
    "\n",
    "我们将为验证集使用批大小，该批大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。我们利用这一优势来使用更大的批量，并更快地计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11249, 20) (11249, 1) (3750, 20) (3750, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_data, Y_data) # 划分训练集和测试集\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将numpy数组转换为PyTorch张量\n",
    "train_x = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
    "test_x = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(train_y).type(torch.FloatTensor)\n",
    "test_y = torch.from_numpy(test_y).type(torch.FloatTensor)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     train_x = train_x.to('cuda')\n",
    "#     test_x = test_x.to('cuda')\n",
    "#     train_y = train_y.to('cuda')\n",
    "#     test_y = test_y.to('cuda')\n",
    "#     print('Using GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PyTorch的TensorDataset和DataLoader将数据集转换为数据加载器\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch)\n",
    "\n",
    "valid_ds = TensorDataset(test_x, test_y)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 准确率计算函数，用于计算预测值和真实值之间的准确率\n",
    "def accuracy(y_pred, y_true):\n",
    "    # 将预测值大于0.5的值转换为1，其余的转换为0\n",
    "    y_pred = (y_pred>0.5).type(torch.IntTensor)\n",
    "    # y_pred = (y_pred>0.5).type(torch.IntTensor).to('cuda')\n",
    "    # 计算预测值和真实值相等的数量的均值，并转换为浮点数\n",
    "    return (y_pred == y_true).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0     loss: 0.564     accuracy: 0.761     test_loss: 0.562     test_accuracy: 0.764\n",
      "epoch: 1     loss: 0.56     accuracy: 0.761     test_loss: 0.559     test_accuracy: 0.764\n",
      "epoch: 2     loss: 0.556     accuracy: 0.761     test_loss: 0.555     test_accuracy: 0.764\n",
      "epoch: 3     loss: 0.552     accuracy: 0.761     test_loss: 0.55     test_accuracy: 0.764\n",
      "epoch: 4     loss: 0.547     accuracy: 0.761     test_loss: 0.545     test_accuracy: 0.764\n",
      "epoch: 5     loss: 0.541     accuracy: 0.761     test_loss: 0.54     test_accuracy: 0.764\n",
      "epoch: 6     loss: 0.535     accuracy: 0.761     test_loss: 0.535     test_accuracy: 0.764\n",
      "epoch: 7     loss: 0.528     accuracy: 0.761     test_loss: 0.527     test_accuracy: 0.764\n",
      "epoch: 8     loss: 0.52     accuracy: 0.761     test_loss: 0.52     test_accuracy: 0.764\n",
      "epoch: 9     loss: 0.512     accuracy: 0.761     test_loss: 0.512     test_accuracy: 0.764\n",
      "epoch: 10     loss: 0.504     accuracy: 0.761     test_loss: 0.504     test_accuracy: 0.764\n",
      "epoch: 11     loss: 0.495     accuracy: 0.761     test_loss: 0.495     test_accuracy: 0.764\n",
      "epoch: 12     loss: 0.486     accuracy: 0.761     test_loss: 0.487     test_accuracy: 0.764\n",
      "epoch: 13     loss: 0.476     accuracy: 0.761     test_loss: 0.477     test_accuracy: 0.764\n",
      "epoch: 14     loss: 0.466     accuracy: 0.76     test_loss: 0.468     test_accuracy: 0.762\n",
      "epoch: 15     loss: 0.456     accuracy: 0.759     test_loss: 0.458     test_accuracy: 0.76\n",
      "epoch: 16     loss: 0.447     accuracy: 0.757     test_loss: 0.449     test_accuracy: 0.759\n",
      "epoch: 17     loss: 0.438     accuracy: 0.764     test_loss: 0.44     test_accuracy: 0.763\n",
      "epoch: 18     loss: 0.428     accuracy: 0.769     test_loss: 0.43     test_accuracy: 0.768\n",
      "epoch: 19     loss: 0.42     accuracy: 0.783     test_loss: 0.422     test_accuracy: 0.781\n",
      "epoch: 20     loss: 0.41     accuracy: 0.789     test_loss: 0.412     test_accuracy: 0.787\n",
      "epoch: 21     loss: 0.402     accuracy: 0.815     test_loss: 0.404     test_accuracy: 0.807\n",
      "epoch: 22     loss: 0.394     accuracy: 0.825     test_loss: 0.396     test_accuracy: 0.818\n",
      "epoch: 23     loss: 0.387     accuracy: 0.836     test_loss: 0.389     test_accuracy: 0.833\n",
      "epoch: 24     loss: 0.381     accuracy: 0.848     test_loss: 0.383     test_accuracy: 0.845\n",
      "epoch: 25     loss: 0.374     accuracy: 0.853     test_loss: 0.377     test_accuracy: 0.849\n",
      "epoch: 26     loss: 0.368     accuracy: 0.855     test_loss: 0.37     test_accuracy: 0.852\n",
      "epoch: 27     loss: 0.362     accuracy: 0.857     test_loss: 0.365     test_accuracy: 0.854\n",
      "epoch: 28     loss: 0.356     accuracy: 0.861     test_loss: 0.359     test_accuracy: 0.858\n",
      "epoch: 29     loss: 0.351     accuracy: 0.863     test_loss: 0.354     test_accuracy: 0.859\n",
      "epoch: 30     loss: 0.345     accuracy: 0.865     test_loss: 0.348     test_accuracy: 0.86\n",
      "epoch: 31     loss: 0.341     accuracy: 0.869     test_loss: 0.344     test_accuracy: 0.865\n",
      "epoch: 32     loss: 0.334     accuracy: 0.871     test_loss: 0.337     test_accuracy: 0.867\n",
      "epoch: 33     loss: 0.33     accuracy: 0.874     test_loss: 0.333     test_accuracy: 0.871\n",
      "epoch: 34     loss: 0.327     accuracy: 0.878     test_loss: 0.331     test_accuracy: 0.877\n",
      "epoch: 35     loss: 0.323     accuracy: 0.88     test_loss: 0.326     test_accuracy: 0.878\n",
      "epoch: 36     loss: 0.32     accuracy: 0.882     test_loss: 0.324     test_accuracy: 0.879\n",
      "epoch: 37     loss: 0.316     accuracy: 0.883     test_loss: 0.32     test_accuracy: 0.88\n",
      "epoch: 38     loss: 0.312     accuracy: 0.884     test_loss: 0.316     test_accuracy: 0.882\n",
      "epoch: 39     loss: 0.309     accuracy: 0.886     test_loss: 0.314     test_accuracy: 0.881\n",
      "epoch: 40     loss: 0.306     accuracy: 0.887     test_loss: 0.311     test_accuracy: 0.884\n",
      "epoch: 41     loss: 0.304     accuracy: 0.888     test_loss: 0.309     test_accuracy: 0.884\n",
      "epoch: 42     loss: 0.301     accuracy: 0.888     test_loss: 0.306     test_accuracy: 0.884\n",
      "epoch: 43     loss: 0.299     accuracy: 0.89     test_loss: 0.304     test_accuracy: 0.886\n",
      "epoch: 44     loss: 0.297     accuracy: 0.891     test_loss: 0.302     test_accuracy: 0.888\n",
      "epoch: 45     loss: 0.295     accuracy: 0.892     test_loss: 0.3     test_accuracy: 0.89\n",
      "epoch: 46     loss: 0.292     accuracy: 0.893     test_loss: 0.298     test_accuracy: 0.889\n",
      "epoch: 47     loss: 0.29     accuracy: 0.894     test_loss: 0.296     test_accuracy: 0.891\n",
      "epoch: 48     loss: 0.288     accuracy: 0.895     test_loss: 0.294     test_accuracy: 0.892\n",
      "epoch: 49     loss: 0.287     accuracy: 0.894     test_loss: 0.293     test_accuracy: 0.891\n",
      "epoch: 50     loss: 0.285     accuracy: 0.895     test_loss: 0.291     test_accuracy: 0.892\n",
      "epoch: 51     loss: 0.284     accuracy: 0.895     test_loss: 0.29     test_accuracy: 0.891\n",
      "epoch: 52     loss: 0.282     accuracy: 0.895     test_loss: 0.289     test_accuracy: 0.892\n",
      "epoch: 53     loss: 0.281     accuracy: 0.895     test_loss: 0.288     test_accuracy: 0.891\n",
      "epoch: 54     loss: 0.279     accuracy: 0.895     test_loss: 0.286     test_accuracy: 0.893\n",
      "epoch: 55     loss: 0.278     accuracy: 0.896     test_loss: 0.285     test_accuracy: 0.893\n",
      "epoch: 56     loss: 0.277     accuracy: 0.896     test_loss: 0.284     test_accuracy: 0.892\n",
      "epoch: 57     loss: 0.275     accuracy: 0.897     test_loss: 0.282     test_accuracy: 0.894\n",
      "epoch: 58     loss: 0.275     accuracy: 0.896     test_loss: 0.282     test_accuracy: 0.893\n",
      "epoch: 59     loss: 0.273     accuracy: 0.897     test_loss: 0.28     test_accuracy: 0.895\n",
      "epoch: 60     loss: 0.273     accuracy: 0.896     test_loss: 0.28     test_accuracy: 0.894\n",
      "epoch: 61     loss: 0.271     accuracy: 0.897     test_loss: 0.279     test_accuracy: 0.896\n",
      "epoch: 62     loss: 0.271     accuracy: 0.897     test_loss: 0.279     test_accuracy: 0.895\n",
      "epoch: 63     loss: 0.27     accuracy: 0.897     test_loss: 0.278     test_accuracy: 0.896\n",
      "epoch: 64     loss: 0.269     accuracy: 0.898     test_loss: 0.277     test_accuracy: 0.897\n",
      "epoch: 65     loss: 0.268     accuracy: 0.898     test_loss: 0.276     test_accuracy: 0.898\n",
      "epoch: 66     loss: 0.266     accuracy: 0.9     test_loss: 0.274     test_accuracy: 0.899\n",
      "epoch: 67     loss: 0.266     accuracy: 0.9     test_loss: 0.274     test_accuracy: 0.899\n",
      "epoch: 68     loss: 0.265     accuracy: 0.898     test_loss: 0.273     test_accuracy: 0.898\n",
      "epoch: 69     loss: 0.264     accuracy: 0.899     test_loss: 0.272     test_accuracy: 0.899\n",
      "epoch: 70     loss: 0.263     accuracy: 0.899     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 71     loss: 0.263     accuracy: 0.9     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 72     loss: 0.263     accuracy: 0.899     test_loss: 0.271     test_accuracy: 0.899\n",
      "epoch: 73     loss: 0.262     accuracy: 0.9     test_loss: 0.27     test_accuracy: 0.899\n",
      "epoch: 74     loss: 0.261     accuracy: 0.901     test_loss: 0.269     test_accuracy: 0.9\n",
      "epoch: 75     loss: 0.261     accuracy: 0.9     test_loss: 0.269     test_accuracy: 0.899\n",
      "epoch: 76     loss: 0.26     accuracy: 0.901     test_loss: 0.268     test_accuracy: 0.901\n",
      "epoch: 77     loss: 0.259     accuracy: 0.901     test_loss: 0.267     test_accuracy: 0.901\n",
      "epoch: 78     loss: 0.259     accuracy: 0.9     test_loss: 0.267     test_accuracy: 0.901\n",
      "epoch: 79     loss: 0.258     accuracy: 0.902     test_loss: 0.266     test_accuracy: 0.902\n",
      "epoch: 80     loss: 0.257     accuracy: 0.902     test_loss: 0.265     test_accuracy: 0.902\n",
      "epoch: 81     loss: 0.256     accuracy: 0.902     test_loss: 0.265     test_accuracy: 0.902\n",
      "epoch: 82     loss: 0.256     accuracy: 0.902     test_loss: 0.264     test_accuracy: 0.901\n",
      "epoch: 83     loss: 0.257     accuracy: 0.9     test_loss: 0.265     test_accuracy: 0.901\n",
      "epoch: 84     loss: 0.255     accuracy: 0.903     test_loss: 0.264     test_accuracy: 0.902\n",
      "epoch: 85     loss: 0.254     accuracy: 0.903     test_loss: 0.263     test_accuracy: 0.903\n",
      "epoch: 86     loss: 0.254     accuracy: 0.902     test_loss: 0.263     test_accuracy: 0.902\n",
      "epoch: 87     loss: 0.254     accuracy: 0.901     test_loss: 0.263     test_accuracy: 0.902\n",
      "epoch: 88     loss: 0.253     accuracy: 0.902     test_loss: 0.262     test_accuracy: 0.903\n",
      "epoch: 89     loss: 0.253     accuracy: 0.902     test_loss: 0.262     test_accuracy: 0.903\n",
      "epoch: 90     loss: 0.253     accuracy: 0.901     test_loss: 0.262     test_accuracy: 0.902\n",
      "epoch: 91     loss: 0.252     accuracy: 0.902     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 92     loss: 0.252     accuracy: 0.901     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 93     loss: 0.252     accuracy: 0.902     test_loss: 0.261     test_accuracy: 0.903\n",
      "epoch: 94     loss: 0.25     accuracy: 0.903     test_loss: 0.26     test_accuracy: 0.904\n",
      "epoch: 95     loss: 0.249     accuracy: 0.903     test_loss: 0.259     test_accuracy: 0.905\n",
      "epoch: 96     loss: 0.249     accuracy: 0.903     test_loss: 0.259     test_accuracy: 0.906\n",
      "epoch: 97     loss: 0.249     accuracy: 0.903     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 98     loss: 0.249     accuracy: 0.903     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 99     loss: 0.248     accuracy: 0.904     test_loss: 0.258     test_accuracy: 0.906\n",
      "epoch: 100     loss: 0.248     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.907\n",
      "epoch: 101     loss: 0.248     accuracy: 0.902     test_loss: 0.258     test_accuracy: 0.905\n",
      "epoch: 102     loss: 0.247     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.906\n",
      "epoch: 103     loss: 0.247     accuracy: 0.904     test_loss: 0.257     test_accuracy: 0.906\n",
      "epoch: 104     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 105     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 106     loss: 0.246     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.906\n",
      "epoch: 107     loss: 0.245     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.907\n",
      "epoch: 108     loss: 0.245     accuracy: 0.905     test_loss: 0.255     test_accuracy: 0.906\n",
      "epoch: 109     loss: 0.245     accuracy: 0.905     test_loss: 0.254     test_accuracy: 0.906\n",
      "epoch: 110     loss: 0.245     accuracy: 0.905     test_loss: 0.254     test_accuracy: 0.907\n",
      "epoch: 111     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.907\n",
      "epoch: 112     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.906\n",
      "epoch: 113     loss: 0.245     accuracy: 0.904     test_loss: 0.254     test_accuracy: 0.906\n",
      "epoch: 114     loss: 0.244     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 115     loss: 0.244     accuracy: 0.904     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 116     loss: 0.243     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.905\n",
      "epoch: 117     loss: 0.243     accuracy: 0.905     test_loss: 0.253     test_accuracy: 0.906\n",
      "epoch: 118     loss: 0.243     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 119     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 120     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 121     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.905\n",
      "epoch: 122     loss: 0.242     accuracy: 0.905     test_loss: 0.252     test_accuracy: 0.906\n",
      "epoch: 123     loss: 0.242     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.905\n",
      "epoch: 124     loss: 0.241     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.907\n",
      "epoch: 125     loss: 0.241     accuracy: 0.905     test_loss: 0.251     test_accuracy: 0.906\n",
      "epoch: 126     loss: 0.241     accuracy: 0.904     test_loss: 0.251     test_accuracy: 0.906\n",
      "epoch: 127     loss: 0.241     accuracy: 0.905     test_loss: 0.25     test_accuracy: 0.906\n",
      "epoch: 128     loss: 0.24     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.906\n",
      "epoch: 129     loss: 0.24     accuracy: 0.905     test_loss: 0.25     test_accuracy: 0.906\n",
      "epoch: 130     loss: 0.24     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.906\n",
      "epoch: 131     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.907\n",
      "epoch: 132     loss: 0.239     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 133     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 134     loss: 0.239     accuracy: 0.906     test_loss: 0.249     test_accuracy: 0.908\n",
      "epoch: 135     loss: 0.24     accuracy: 0.905     test_loss: 0.249     test_accuracy: 0.907\n",
      "epoch: 136     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.909\n",
      "epoch: 137     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.908\n",
      "epoch: 138     loss: 0.237     accuracy: 0.907     test_loss: 0.247     test_accuracy: 0.91\n",
      "epoch: 139     loss: 0.238     accuracy: 0.906     test_loss: 0.248     test_accuracy: 0.908\n",
      "epoch: 140     loss: 0.238     accuracy: 0.906     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 141     loss: 0.237     accuracy: 0.906     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 142     loss: 0.237     accuracy: 0.907     test_loss: 0.247     test_accuracy: 0.909\n",
      "epoch: 143     loss: 0.237     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.908\n",
      "epoch: 144     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.909\n",
      "epoch: 145     loss: 0.236     accuracy: 0.908     test_loss: 0.246     test_accuracy: 0.909\n",
      "epoch: 146     loss: 0.237     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 147     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 148     loss: 0.236     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 149     loss: 0.236     accuracy: 0.907     test_loss: 0.246     test_accuracy: 0.907\n",
      "epoch: 150     loss: 0.236     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.908\n",
      "epoch: 151     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 152     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.909\n",
      "epoch: 153     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.909\n",
      "epoch: 154     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.909\n",
      "epoch: 155     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 156     loss: 0.235     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 157     loss: 0.234     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 158     loss: 0.235     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 159     loss: 0.234     accuracy: 0.908     test_loss: 0.244     test_accuracy: 0.907\n",
      "epoch: 160     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.907\n",
      "epoch: 161     loss: 0.235     accuracy: 0.908     test_loss: 0.245     test_accuracy: 0.907\n",
      "epoch: 162     loss: 0.234     accuracy: 0.909     test_loss: 0.244     test_accuracy: 0.908\n",
      "epoch: 163     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.908\n",
      "epoch: 164     loss: 0.234     accuracy: 0.909     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 165     loss: 0.234     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 166     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 167     loss: 0.233     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.909\n",
      "epoch: 168     loss: 0.233     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.907\n",
      "epoch: 169     loss: 0.234     accuracy: 0.908     test_loss: 0.243     test_accuracy: 0.907\n",
      "epoch: 170     loss: 0.233     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.908\n",
      "epoch: 171     loss: 0.232     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.909\n",
      "epoch: 172     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 173     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 174     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 175     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 176     loss: 0.232     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 177     loss: 0.232     accuracy: 0.909     test_loss: 0.242     test_accuracy: 0.909\n",
      "epoch: 178     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.91\n",
      "epoch: 179     loss: 0.231     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.91\n",
      "epoch: 180     loss: 0.232     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 181     loss: 0.232     accuracy: 0.909     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 182     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 183     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 184     loss: 0.231     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 185     loss: 0.23     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 186     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.909\n",
      "epoch: 187     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.91\n",
      "epoch: 188     loss: 0.231     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 189     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.91\n",
      "epoch: 190     loss: 0.231     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 191     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 192     loss: 0.231     accuracy: 0.91     test_loss: 0.241     test_accuracy: 0.908\n",
      "epoch: 193     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 194     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.909\n",
      "epoch: 195     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 196     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 197     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 198     loss: 0.23     accuracy: 0.911     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 199     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 200     loss: 0.23     accuracy: 0.91     test_loss: 0.24     test_accuracy: 0.908\n",
      "epoch: 201     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 202     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 203     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 204     loss: 0.229     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 205     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 206     loss: 0.229     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.91\n",
      "epoch: 207     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.908\n",
      "epoch: 208     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.909\n",
      "epoch: 209     loss: 0.23     accuracy: 0.911     test_loss: 0.239     test_accuracy: 0.909\n",
      "epoch: 210     loss: 0.229     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 211     loss: 0.228     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 212     loss: 0.229     accuracy: 0.911     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 213     loss: 0.229     accuracy: 0.911     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 214     loss: 0.228     accuracy: 0.912     test_loss: 0.238     test_accuracy: 0.909\n",
      "epoch: 215     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 216     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 217     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 218     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 219     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 220     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 221     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 222     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 223     loss: 0.228     accuracy: 0.912     test_loss: 0.237     test_accuracy: 0.91\n",
      "epoch: 224     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 225     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 226     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 227     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 228     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.911\n",
      "epoch: 229     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.91\n",
      "epoch: 230     loss: 0.226     accuracy: 0.912     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 231     loss: 0.226     accuracy: 0.912     test_loss: 0.235     test_accuracy: 0.911\n",
      "epoch: 232     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 233     loss: 0.227     accuracy: 0.912     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 234     loss: 0.227     accuracy: 0.913     test_loss: 0.236     test_accuracy: 0.909\n",
      "epoch: 235     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 236     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 237     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 238     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 239     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 240     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 241     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.911\n",
      "epoch: 242     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 243     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.911\n",
      "epoch: 244     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 245     loss: 0.226     accuracy: 0.913     test_loss: 0.235     test_accuracy: 0.91\n",
      "epoch: 246     loss: 0.225     accuracy: 0.914     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 247     loss: 0.225     accuracy: 0.913     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 248     loss: 0.225     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 249     loss: 0.225     accuracy: 0.913     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 250     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 251     loss: 0.225     accuracy: 0.913     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 252     loss: 0.225     accuracy: 0.914     test_loss: 0.234     test_accuracy: 0.91\n",
      "epoch: 253     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 254     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 255     loss: 0.225     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 256     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 257     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 258     loss: 0.224     accuracy: 0.914     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 259     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 260     loss: 0.224     accuracy: 0.915     test_loss: 0.233     test_accuracy: 0.91\n",
      "epoch: 261     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 262     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 263     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 264     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.909\n",
      "epoch: 265     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 266     loss: 0.224     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.91\n",
      "epoch: 267     loss: 0.223     accuracy: 0.915     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 268     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 269     loss: 0.223     accuracy: 0.915     test_loss: 0.232     test_accuracy: 0.911\n",
      "epoch: 270     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 271     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 272     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 273     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 274     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 275     loss: 0.223     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 276     loss: 0.222     accuracy: 0.916     test_loss: 0.231     test_accuracy: 0.912\n",
      "epoch: 277     loss: 0.223     accuracy: 0.915     test_loss: 0.231     test_accuracy: 0.911\n",
      "epoch: 278     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 279     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 280     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 281     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 282     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 283     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 284     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 285     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.913\n",
      "epoch: 286     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 287     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 288     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 289     loss: 0.222     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 290     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 291     loss: 0.222     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 292     loss: 0.221     accuracy: 0.917     test_loss: 0.23     test_accuracy: 0.912\n",
      "epoch: 293     loss: 0.221     accuracy: 0.916     test_loss: 0.23     test_accuracy: 0.911\n",
      "epoch: 294     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.912\n",
      "epoch: 295     loss: 0.22     accuracy: 0.918     test_loss: 0.229     test_accuracy: 0.914\n",
      "epoch: 296     loss: 0.22     accuracy: 0.918     test_loss: 0.229     test_accuracy: 0.914\n",
      "epoch: 297     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.912\n",
      "epoch: 298     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.914\n",
      "epoch: 299     loss: 0.221     accuracy: 0.917     test_loss: 0.229     test_accuracy: 0.913\n",
      "epoch: 300     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 301     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 302     loss: 0.22     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 303     loss: 0.22     accuracy: 0.917     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 304     loss: 0.22     accuracy: 0.917     test_loss: 0.228     test_accuracy: 0.914\n",
      "epoch: 305     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 306     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 307     loss: 0.219     accuracy: 0.918     test_loss: 0.228     test_accuracy: 0.913\n",
      "epoch: 308     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 309     loss: 0.219     accuracy: 0.919     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 310     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.914\n",
      "epoch: 311     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 312     loss: 0.219     accuracy: 0.918     test_loss: 0.227     test_accuracy: 0.915\n",
      "epoch: 313     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 314     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 315     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 316     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 317     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 318     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 319     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 320     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.915\n",
      "epoch: 321     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 322     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 323     loss: 0.218     accuracy: 0.919     test_loss: 0.226     test_accuracy: 0.914\n",
      "epoch: 324     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 325     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 326     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 327     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 328     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 329     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 330     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.916\n",
      "epoch: 331     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 332     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 333     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 334     loss: 0.217     accuracy: 0.919     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 335     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 336     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 337     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 338     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 339     loss: 0.217     accuracy: 0.92     test_loss: 0.225     test_accuracy: 0.915\n",
      "epoch: 340     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 341     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 342     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 343     loss: 0.217     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 344     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.915\n",
      "epoch: 345     loss: 0.216     accuracy: 0.92     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 346     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 347     loss: 0.216     accuracy: 0.921     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 348     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 349     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 350     loss: 0.216     accuracy: 0.92     test_loss: 0.224     test_accuracy: 0.916\n",
      "epoch: 351     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 352     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 353     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 354     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 355     loss: 0.215     accuracy: 0.921     test_loss: 0.223     test_accuracy: 0.917\n",
      "epoch: 356     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 357     loss: 0.215     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 358     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 359     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 360     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 361     loss: 0.214     accuracy: 0.921     test_loss: 0.222     test_accuracy: 0.917\n",
      "epoch: 362     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 363     loss: 0.214     accuracy: 0.922     test_loss: 0.222     test_accuracy: 0.918\n",
      "epoch: 364     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 365     loss: 0.214     accuracy: 0.922     test_loss: 0.222     test_accuracy: 0.918\n",
      "epoch: 366     loss: 0.214     accuracy: 0.921     test_loss: 0.221     test_accuracy: 0.917\n",
      "epoch: 367     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 368     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 369     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 370     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 371     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 372     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 373     loss: 0.213     accuracy: 0.921     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 374     loss: 0.214     accuracy: 0.922     test_loss: 0.221     test_accuracy: 0.918\n",
      "epoch: 375     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 376     loss: 0.213     accuracy: 0.922     test_loss: 0.221     test_accuracy: 0.919\n",
      "epoch: 377     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 378     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 379     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 380     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 381     loss: 0.213     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 382     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 383     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 384     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.919\n",
      "epoch: 385     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 386     loss: 0.212     accuracy: 0.922     test_loss: 0.22     test_accuracy: 0.918\n",
      "epoch: 387     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 388     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 389     loss: 0.211     accuracy: 0.923     test_loss: 0.219     test_accuracy: 0.92\n",
      "epoch: 390     loss: 0.211     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 391     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 392     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 393     loss: 0.211     accuracy: 0.923     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 394     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 395     loss: 0.212     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.92\n",
      "epoch: 396     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 397     loss: 0.211     accuracy: 0.922     test_loss: 0.219     test_accuracy: 0.919\n",
      "epoch: 398     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 399     loss: 0.211     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 400     loss: 0.211     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 401     loss: 0.21     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 402     loss: 0.21     accuracy: 0.923     test_loss: 0.218     test_accuracy: 0.919\n",
      "epoch: 403     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 404     loss: 0.21     accuracy: 0.922     test_loss: 0.218     test_accuracy: 0.92\n",
      "epoch: 405     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 406     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 407     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 408     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 409     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 410     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 411     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 412     loss: 0.21     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 413     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.92\n",
      "epoch: 414     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 415     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 416     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 417     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.92\n",
      "epoch: 418     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 419     loss: 0.209     accuracy: 0.923     test_loss: 0.217     test_accuracy: 0.921\n",
      "epoch: 420     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 421     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.921\n",
      "epoch: 422     loss: 0.209     accuracy: 0.924     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 423     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.921\n",
      "epoch: 424     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 425     loss: 0.209     accuracy: 0.923     test_loss: 0.216     test_accuracy: 0.921\n",
      "epoch: 426     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 427     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 428     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 429     loss: 0.208     accuracy: 0.924     test_loss: 0.215     test_accuracy: 0.922\n",
      "epoch: 430     loss: 0.208     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 431     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 432     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 433     loss: 0.208     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 434     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 435     loss: 0.207     accuracy: 0.923     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 436     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 437     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 438     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 439     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 440     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 441     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 442     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 443     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.922\n",
      "epoch: 444     loss: 0.207     accuracy: 0.924     test_loss: 0.214     test_accuracy: 0.923\n",
      "epoch: 445     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 446     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 447     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 448     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 449     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 450     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.924\n",
      "epoch: 451     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 452     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 453     loss: 0.207     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 454     loss: 0.206     accuracy: 0.924     test_loss: 0.213     test_accuracy: 0.923\n",
      "epoch: 455     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 456     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 457     loss: 0.206     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 458     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 459     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 460     loss: 0.205     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 461     loss: 0.205     accuracy: 0.924     test_loss: 0.212     test_accuracy: 0.923\n",
      "epoch: 462     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.923\n",
      "epoch: 463     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 464     loss: 0.205     accuracy: 0.924     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 465     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 466     loss: 0.205     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 467     loss: 0.204     accuracy: 0.925     test_loss: 0.211     test_accuracy: 0.924\n",
      "epoch: 468     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 469     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 470     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 471     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 472     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 473     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 474     loss: 0.203     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 475     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 476     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 477     loss: 0.204     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.925\n",
      "epoch: 478     loss: 0.203     accuracy: 0.925     test_loss: 0.21     test_accuracy: 0.924\n",
      "epoch: 479     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 480     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 481     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 482     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 483     loss: 0.203     accuracy: 0.926     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 484     loss: 0.203     accuracy: 0.926     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 485     loss: 0.203     accuracy: 0.925     test_loss: 0.209     test_accuracy: 0.925\n",
      "epoch: 486     loss: 0.203     accuracy: 0.925     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 487     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 488     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 489     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 490     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 491     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.926\n",
      "epoch: 492     loss: 0.202     accuracy: 0.926     test_loss: 0.208     test_accuracy: 0.925\n",
      "epoch: 493     loss: 0.202     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 494     loss: 0.202     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 495     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 496     loss: 0.201     accuracy: 0.927     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 497     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 498     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n",
      "epoch: 499     loss: 0.201     accuracy: 0.926     test_loss: 0.207     test_accuracy: 0.926\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model, opt = get_model()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        epoch_accuracy = accuracy(model(train_x), train_y).data.item()\n",
    "        epoch_loss = loss_fn(model(train_x), train_y).data.item()\n",
    "\n",
    "        epoch_test_accuracy = accuracy(model(test_x), test_y).data.item()\n",
    "        epoch_test_loss = loss_fn(model(test_x), test_y).data.item()\n",
    "        print('epoch:', epoch, '   ', 'loss:', round(epoch_loss, 3),\n",
    "                               '   ', 'accuracy:', round(epoch_accuracy, 3),\n",
    "                               '   ', 'test_loss:', round(epoch_test_loss, 3),\n",
    "                               '   ', 'test_accuracy:', round(epoch_test_accuracy, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
