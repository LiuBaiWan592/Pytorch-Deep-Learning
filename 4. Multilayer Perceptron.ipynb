{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知器简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 二分类问题的逻辑回归模型是单个神经元：\n",
    "* 计算输入特征的加权和\n",
    "* 然后使用一个激活函数（或传递函数）计算输出（信用卡欺诈问题中使用的激活函数为Sigmoid函数）  \n",
    "*单层的多分类问题需要多个神经元，但与二分类问题都为单层神经元模型*\n",
    "\n",
    "2. 单层神经元的缺陷：\n",
    "* 拟合能力不够强，无法拟合“异或”运算\n",
    "* 单层神经元要求数据是线性可分的，“异或”问题无法找到一条直线分割两个类\n",
    "\n",
    "3. 多层感知器\n",
    "* 模仿大脑神经元连接方式，将多层神经元进行连接，加深模型深度\n",
    "* 在每一层中增加激活函数，给模型带来了非线性，使模型的拟合能力大大增强\n",
    "* 激活函数可对神经元的输入进行判断，从而选择输出方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid激活函数\n",
    "* Sigmoid(x) = 1 / [1 + e ^ (-x)] （将 (-∞, +∞) 映射到 (0, 1) ）\n",
    "* Sigmoid函数是一个概率分布函数，给定某个输入，将输出为一个概率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sigmoid优点**\n",
    "\n",
    "* 输出范围明确：Sigmoid函数的输出范围在0到1之间，非常适合作为模型的输出函数。用于输出一个0到1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。\n",
    "* 便于求导：梯度平滑，便于求导，防止模型训练过程中出现突变的梯度。\n",
    "\n",
    "**sigmoid缺点**\n",
    "\n",
    "* 梯度消失：导函数图像中，sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新。\n",
    "* 非零中心化输出：Sigmoid函数的输出不是以0为中心的，而是以0.5为中心。这意味着在训练过程中，输出值总是偏向正值，可能导致权重更新偏向于一个方向，会呈Z型梯度下降，影响学习效率。\n",
    "* 饱和性：Sigmoid函数的饱和性导致其在输入值的极端情况下对输入变化不敏感，这限制了网络对极端值的学习能力。\n",
    "* 计算资源消耗：Sigmoid函数涉及指数运算，这在计算上可能比其他一些激活函数（如ReLU）更加耗时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3304, -0.6195,  0.8765, -0.8322,  0.1955])\n",
      "tensor([0.7909, 0.3499, 0.7061, 0.3032, 0.5487])\n"
     ]
    }
   ],
   "source": [
    "# 演示Sigmoid激活函数\n",
    "input = torch.randn(5)\n",
    "print(input)\n",
    "# Sigmoid activation function\n",
    "output = torch.sigmoid(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tanh激活函数\n",
    "* 双曲正切激活函数，实际上是Sigmoid激活函数的变形\n",
    "* Tanh(x) = [e ^ (x) - e ^ (-x)] / [e ^ (x) + e ^ (-x)] = 2 * Sigmoid(2 * x) - 1 （将 (-∞, +∞) 映射到 (-1, 1) ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanh优点：**\n",
    "* 与sigmoid相比，多了一个零中心化输出。这有助于数据的稳定性和收敛性，因为它可以减少学习过程中的偏移。\n",
    "\n",
    "**Tanh缺点：**\n",
    "* 梯度消失问题：尽管Tanh函数在输入接近0时的梯度较大，但在输入值非常大或非常小的情况下，Tanh函数的导数仍然会接近0，导致梯度消失问题。\n",
    "* 计算资源消耗：Tanh函数涉及指数运算，这可能比其他一些激活函数（如ReLU）在计算上更加耗时。\n",
    "* 初始化敏感性：Tanh函数对权重初始化较为敏感，如果权重初始化不当，可能会导致梯度消失或爆炸问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1437,  0.5809, -0.4289,  0.2251, -0.7335])\n",
      "tensor([ 0.8157,  0.5234, -0.4044,  0.2214, -0.6252])\n"
     ]
    }
   ],
   "source": [
    "# 演示Tanh激活函数\n",
    "input = torch.randn(5)\n",
    "print(input)\n",
    "# Tanh activation function\n",
    "output = torch.tanh(input)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
